{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised Learning 2 - Other techniques (on Tileset7) - Aug 2017\n",
    "Created:  16 Aug 2018 <br>\n",
    "Last update: 24 Aug 2018 (small changes, but same results)\n",
    "\n",
    "\n",
    "### Use some more unsupervised techniques learned from DataCamp\n",
    "\n",
    "This continues the work from 'realxtals1-unsupervised1.ipynb'. Some of the functions of that notebook have now been moved into imgutils plus some other extensions in imgutils visualization (mostly adding the 'large heatmap' capability and extra annotations)\n",
    "\n",
    "About the data: The data used here has been prepared in my prior notebooks. It's a bunch of images (from a larger 'tile set') sliced up in sub-images and image statistics applied on each sub-image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2. Import Crystal Image Data & Statistics\n",
    "The data was labeled and exported to csv in the notebook realxtals1_dataeng1.ipynb\n",
    "\n",
    "#### About the data:\n",
    "The CSV contains the image files, slice information (sub-images) and associated statistics, which are the features for which a classifier needs to be found. \n",
    "\n",
    "The goal is to find the clustering in feature-space and use those to categorize the images. For this particular dataset, a single statistics could be used to label into three classes:<br>\n",
    "\n",
    "A = subimage contains no crystal, <br>\n",
    "B = part of subimage contains crystal, <br>\n",
    "C = (most of) subimage contains crystal\n",
    "\n",
    "But the labels have been added here for analyses, eventually the data will be unlabelled.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Crystals_Apr_12/Tileset7-2.csv', sep=';')\n",
    "df.head(3)\n",
    "\n",
    "imgnames = df['filename'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 3. Re-do some of the clustering from previous notebook\n",
    "\n",
    "(so we have some comparison material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First vectorize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into X Y vectors:\n",
    "feature_cols = ['|img_std|', '|img_std2|', '|img_mean|','|img_skewness|', '|img_kurtosis|','|img_mode|']\n",
    "X = df.loc[:,feature_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = cluster.KMeans(algorithm='auto', n_clusters=number_of_clusters, n_init=10, init='k-means++')\n",
    "k_means_pred = k_means.fit_predict(X)\n",
    "print(\"score (silhouette): \", silhouette_score(X, k_means_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Affinity = {“euclidean”, “l1”, “l2”, “manhattan”, “cosine”}\n",
    "# Linkage = {“ward”, “complete”, “average”}\n",
    "\n",
    "Hclustering = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')\n",
    "hierarch_pred = Hclustering.fit_predict(X)\n",
    "print(\"score (silhouette): \", silhouette_score(X, hierarch_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral and DBScan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = cluster.SpectralClustering(n_clusters=number_of_clusters,eigen_solver='arpack',affinity=\"nearest_neighbors\")\n",
    "spectral_pred = spectral.fit_predict(X)\n",
    "print(\"score (silhouette): \", silhouette_score(X, spectral_pred))\n",
    "\n",
    "dbscan = cluster.DBSCAN(eps=0.5, metric='euclidean', min_samples=10)\n",
    "dbscan_pred = dbscan.fit_predict(X)\n",
    "print(\"score (silhouette): \", silhouette_score(X, dbscan_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also get the PCA transformed data and k-means and hierach with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "fieldnames = ['pca_1','pca_2','pca_3', 'pca_4', 'pca_5']\n",
    "\n",
    "n_comp = 5;\n",
    "\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "\n",
    "# convert into X Y vectors:\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_pca = cluster.KMeans(algorithm='auto', n_clusters=3, n_init=10, init='k-means++')\n",
    "k_means_pca_pred = k_means_pca.fit_predict(X_pca)\n",
    "print(\"score (silhouette): \", silhouette_score(X_pca, k_means_pca_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hclustering_pca = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')\n",
    "hierarch_pca_pred = Hclustering_pca.fit_predict(X_pca)\n",
    "print(\"score (silhouette): \", silhouette_score(X, hierarch_pca_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dummy'] = 0\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgnames[0:6], n_rows=2, n_cols=3, fig_size=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 4. Visualize  k-means and hierarchical clustering  and assess scores\n",
    "\n",
    "(The large heat map is now part of imgutils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unsupervised clustering results to the dataframe\n",
    "df3 = df\n",
    "df3['k_means'] = k_means_pred\n",
    "df3['hierarch'] = hierarch_pred\n",
    "\n",
    "figsize=(8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show heatmaps:\n",
    "imgutils.show_large_heatmap(df3, 'k_means', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'hierarch', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The baseline score of these two with manual counting\n",
    "\n",
    "(see previous notebook (unsupervised1). The idea is to count true positives and false positives on the important categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_imgs_per_class(df_imgstats, classcolumn):\n",
    "    return df_imgstats[classcolumn].value_counts()\n",
    "\n",
    "def print_scores(methodname, class_count_tuples): \n",
    "    \"\"\"\n",
    "    the class tuple has form (classname, n_true_pos, n_false_pos, n_real_pos)\n",
    "    \"\"\"\n",
    "    print(\"\")\n",
    "    print(\"{:<20}|{:^12}|{:^12}|\".format(methodname.upper(), \"True Pos\", \"False Pos\"))\n",
    "    print(\"-\"*(20+12+12+3))\n",
    "    \n",
    "    def print_score_line(class_name, TPR, FDR):\n",
    "        print(\"{:<20}|{:^12.2%}|{:^12.2%}| \".format(class_name, TPR, FDR ))  \n",
    "    \n",
    "    for (class_name, n_true_pos, n_false_pos, n_real_pos) in class_count_tuples:\n",
    "        TPR = n_true_pos/n_real_pos\n",
    "        FDR = n_false_pos/(n_true_pos + n_false_pos)\n",
    "        print_score_line(class_name, TPR, FDR)\n",
    "    \n",
    "    print(\"-\"*(20+12+12+3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores('Manual (using STD)', [('Full Crystal', 11, 2, 11),  ('Partial Crystal', 6, 4, 8) ])\n",
    "print_scores('Hierarchical', [('Full Crystal', 11, 1, 11), ('Partial Crystal', 7, 12, 8) ])\n",
    "print_scores('K-means', [('Full Crystal', 10, 1, 11), ('Partial Crystal', 7, 13, 8) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**: \n",
    "- some of the false positives in category 'Partial' are 'Full Ones' and some false positives in 'Full' are partial ones. So this score is a bit to strict, but accounting for this would require more complex scoring (or a full confusion matrix, where you still need to remark that some confusion is not so critical)\n",
    "- running the algorithms gives some variation, so these scores may deviate a bit (it is manually counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 5. 'Unsupervise scoring' based on cluster 'shape' (instead of using ground truth labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette-score assesses 'cluster consistency' in a single number, and is part of sklearn package \n",
    "see https://en.wikipedia.org/wiki/Silhouette_(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(name, data, labels):\n",
    "    print(\"%s: %f\" % (name,silhouette_score(data, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for k-means, spectral, dbscan and hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score('k-means', X, k_means_pred)\n",
    "print_score('spectral', X, spectral_pred)\n",
    "print_score('dbscan', X, dbscan_pred)\n",
    "print_score('hierarchical', X, hierarch_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the pca variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score('k-means PCA', X, k_means_pca_pred)\n",
    "print_score('k-means PCA2', X_pca, k_means_pca_pred)\n",
    "print_score('hierarchical PCA', X, hierarch_pca_pred)\n",
    "print_score('hierarchical PCA2', X_pca, hierarch_pca_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the intrinsic scoring slighlty prefers k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 6. Check some other interesting properties (from DataCamp)\n",
    "\n",
    "### Examine PCA components importance, with the 'variance explained' of the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ex = pca.explained_variance_ratio_\n",
    "print(var_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to point out 'elbow', but from the values indeed the first two or three are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  And look at the correlation between statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"std - std2:\", pearsonr(df['|img_std|'], df['|img_std2|']))\n",
    "print(\"std - mean:\", pearsonr(df['|img_std|'], df['|img_mean|']))\n",
    "print(\"mean - kurtosis:\", pearsonr(df['|img_mean|'], df['|img_kurtosis|']))\n",
    "print(\"mean - skewness:\", pearsonr(df['|img_mean|'], df['|img_skewness|']))\n",
    "print(\"kurtosis - skewness:\", pearsonr(df['|img_kurtosis|'], df['|img_skewness|']))\n",
    "print(\"mean - mode:\", pearsonr(df['|img_mean|'], df['|img_mode|']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the first value is the correction coefficient, second a p-value; see https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.pearsonr.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, std and std2 are highly correlated.  Interesting/puzzling that they can be combined to identify clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between PCA components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca_1 - pca_2:\", pearsonr(df_pca['pca_1'], df_pca['pca_2']))\n",
    "print(\"pca_1 - pca_3:\", pearsonr(df_pca['pca_1'], df_pca['pca_3']))\n",
    "print(\"pca_2 - pca_3:\", pearsonr(df_pca['pca_2'], df_pca['pca_3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, as expected, pca components are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 7.NMF Similarity (Non-Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is as follows:\n",
    "* standardize the data in such a way that is has no zero values\n",
    "* apply NMF\n",
    "* normalize each sample (so their feature vector has lenght 1),\n",
    "* pick one sample as the reference\n",
    "* perform a dot product of all samples with the reference samples;\n",
    "* if feature vectors are very similar, the dot product will be close to one\n",
    "\n",
    "If we pick a sub image with clear x-crystals as the refernece, see how well this dot product works for a heat map\n",
    "\n",
    "More info on NMF: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html (and on DataCamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skpreproc\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# assure the all data is non negative and in sane rane\n",
    "X_scaled = skpreproc.minmax_scale(X)\n",
    "\n",
    "nmf = NMF()\n",
    "X_nmf = nmf.fit_transform(X_scaled)\n",
    "\n",
    "# for feature comparison, each feature vector should be normalized (i.e. per sample)\n",
    "X_nmf_norm = skpreproc.normalize(X_nmf)\n",
    "\n",
    "df_nmf = pd.DataFrame(X_nmf_norm)\n",
    "df_nmf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select a reference tile from the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)\n",
    "\n",
    "img1, dummy = imgutils.getimgslices_fromdf(df, imgnames[0])\n",
    "imgutils.showimgs(img1, tile_labels=True, fig_size=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use tile (2,3) as the reference image 'with crystal'; need to determine it's row number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['filename']==imgnames[0]) & (df['s_y']==2) &  (df['s_x']==3)].iloc[:,2:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, that is why I introduced alias, so I can just use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['alias']==\"img0_2-3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, it's row 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_vect = X_nmf_norm[11]\n",
    "similarities = df_nmf.dot(ref_vect)\n",
    "\n",
    "# check if indeed similarity of row 11 is 1 \n",
    "print(similarities[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign this to the dataframe and then use this as heats\n",
    "df3['similarity_img0-2-3'] = similarities\n",
    "imgutils.show_large_heatmap(df3, 'similarity_img0-2-3', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, hard to assess, let's map to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['|sim_img0-2-3|'] = df3['similarity_img0-2-3'].map(lambda x: int(x * 3 - 0.0001))\n",
    "df3['|sim_img0-2-3|'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, '|sim_img0-2-3|', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, color coding can be misleading. What if I just define two clusters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['|sim_img0-2-3b|'] = df3['similarity_img0-2-3'].map(lambda x: int(x * 2 - 0.0001))\n",
    "df3['|sim_img0-2-3b|'].value_counts()\n",
    "imgutils.show_large_heatmap(df3, '|sim_img0-2-3b|', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Let's also try without NMF decomposition (just similarity via dot product of original feature vectors). But first assess scores.\n",
    "\n",
    "### Clustering score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can assess the clustering in either 'original space' or in NMF transformed space:\n",
    "print_score('NMF (original)', X, df3['|sim_img0-2-3|'] )\n",
    "print_score('NMF Transformed', X_nmf, df3['|sim_img0-2-3|'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very high..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 8. Similarity without any transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = skpreproc.normalize(X) # the orignal features, but normalized per sample\n",
    "ref_vect2 = X_norm[11]  \n",
    "df3['sim2_img0-2-3'] = X_norm.dot(ref_vect2)\n",
    "\n",
    "# do a quick check; element 11 should have value 1\n",
    "df3['sim2_img0-2-3'].iloc[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'sim2_img0-2-3', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks actually pretty good. For a heatmap the gradual scale is nice, but for classification we need to reduce this to e.g. 3 clusters to compare it's intrinsic score to the other approaches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we observed negatives, we are going to make again 3 categories\n",
    "# but note that range is now -1 to +1,\n",
    "df3['|sim2_img0-2-3|'] = df3['sim2_img0-2-3'].map(lambda x: int( (x+1) * 3 / 2 - 0.0001))\n",
    "df3['|sim2_img0-2-3|'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, '|sim2_img0-2-3|', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the clustering in 'original space' and 'rescaled space':\n",
    "print_score('Similarity (original)', X, df3['|sim2_img0-2-3|'] )\n",
    "print_score('Similarity (scaled)', X_norm, df3['|sim2_img0-2-3|'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to score this would be using the assessment from the heatmap, which involves counting :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores('Similarity', [('Full Crystal', 11, 1, 11),  ('Partial Crystal', 7, 13, 8) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the false positive of the full-crystal is debatable.\n",
    "\n",
    "In general, I should maybe even aim for a more binary classification with or without)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 9. Similarity based on PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the PCA features per sample so we can compare the feature vectors\n",
    "X_pca_norm = skpreproc.normalize(X_pca) \n",
    "\n",
    "# compare via dot product with the reference image\n",
    "ref_vect3 = X_pca_norm[11]  \n",
    "df3['sim3_img0-2-3'] = X_pca_norm.dot(ref_vect3)\n",
    "\n",
    "# do a quick check; element 11 should have value 1\n",
    "df3['sim3_img0-2-3'].iloc[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'sim3_img0-2-3', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good; let's also try when truncated to 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust = 3\n",
    "df3['|sim3_img0-2-3|'] = df3['sim3_img0-2-3'].map(lambda x: int( ((x+1)/2) * nclust  - 0.0001))\n",
    "df3['|sim3_img0-2-3|'].value_counts()\n",
    "\n",
    "imgutils.show_large_heatmap(df3, '|sim3_img0-2-3|', imgnames[0:6], n_rows=2, n_cols=3, fig_size=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score (silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the clustering score in 'original space' and 'pca space':\n",
    "print_score('PCA Similarity (original)', X, df3['|sim3_img0-2-3|'] )\n",
    "print_score('PCA Similarity (pca)', X_pca, df3['|sim3_img0-2-3|'] )\n",
    "print_score('PCA Similarity (normalized pca)', X_pca_norm, df3['|sim3_img0-2-3|'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with similarity approach to cluster into 3 groups, gives a clustering score of ~ 0.42 in original or PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 10. Conclusions\n",
    "\n",
    "* Via extra study and the DataCamp courses, I learned a few new techniques which I utilized here.\n",
    "* One of them is a scoring based on the clusters via **silhouette scoring** , which does not require\n",
    "labelling\n",
    "* An alternate unsupervised learning technique is **NMF (Non-negative Matrix Factorization)** plus feature vector similarity;\n",
    "(often this is used for texts or for images (but than based on the pixel values)\n",
    "* The **NMF approach did not work** well for this data set\n",
    "* However, using the same **similarity vector** approach ** on original statistics works well** (on this dataset)\n",
    "* Applying the similarity approach on the PCA transformed statistics gave similar results\n",
    "\n",
    "In other words: **(simple) vector similarity can be a good alternative for the heat maps**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps:\n",
    "* try whole pipeline on the harder set\n",
    "* consider chaining steps together via sklearn.pipeline\n",
    "\n",
    "Michael Janus, 16 August 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
