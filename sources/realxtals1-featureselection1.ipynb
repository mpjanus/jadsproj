{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Selection (on Tileset7) - July 2017\n",
    "Created:  17 July 2018 <br>\n",
    "Last update: 17 july 2018\n",
    "\n",
    "\n",
    "### Using random forest  (extra forest), existing features can be ranked in order of contribution\n",
    "(This notebook follows the feature selection notebook from Pierluggi)\n",
    "\n",
    "Note that this is a different approach then PCA (see notebook 'realxtals1-dimensionality1').\n",
    "In PCA, the data is transformed onto the 'natural axis' of the data (its eigen vectors) and the top N of these are used, while in feature selection the existing features are being assessed based on their contribution to a classifier.\n",
    "\n",
    "See e.g.:\n",
    "* https://www.quora.com/What-is-the-difference-between-principal-component-analysis-PCA-and-feature-selection-in-machine-learning-Is-PCA-a-means-of-feature-selection\n",
    "* https://stats.stackexchange.com/questions/182711/principal-component-analysis-vs-feature-selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Crystal Image Data & Statistics\n",
    "The data was labeled and exported to csv in the notebook realxtals1_dataeng1.ipynb\n",
    "\n",
    "#### About the data:\n",
    "The CSV contains the image files, slice information (sub-images) and associated statistics, which are the features for which a classifier needs to be found. \n",
    "\n",
    "The goal is to find the clustering in feature-space and use those to categorize the images. For this particular dataset, a single statistics could be used to label into three classes:<br>\n",
    "\n",
    "A = subimage contains no crystal, <br>\n",
    "B = part of subimage contains crystal, <br>\n",
    "C = (most of) subimage contains crystal\n",
    "\n",
    "But the labels have been added here for analyses, eventually the data will be unlabelled.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Crystals_Apr_12/Tileset7-2.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick visual inspection of the 'feature space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it in 3 dimensions, choosing some stat combinations\n",
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 - Exploring feature space\",fontsize=14)\n",
    "\n",
    "# trick to convert category labels into color codes\n",
    "color = pd.DataFrame(df['class'].astype('category'))['class'].cat.codes\n",
    "\n",
    "def scatter_3d(ax, df, feat1, feat2, feat3, colors):\n",
    "    ax.scatter(df[feat1], df[feat2], df[feat3], c=colors)\n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.set_zlabel(feat3)\n",
    "\n",
    "\n",
    "ax = fig0.add_subplot(221, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)\n",
    "\n",
    "ax = fig0.add_subplot(222, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_kurtosis|', '|img_skewness|', color)\n",
    "\n",
    "ax = fig0.add_subplot(223, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mode|', '|img_kurtosis|', '|img_std|', color)\n",
    "\n",
    "ax = fig0.add_subplot(224, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_mode|', '|img_std|', color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual inspection of these graphs, I would expect the std, std2 and kurtosis to be the most important features (they seem like the most seperating ones, though the mean has quite a big variance). Also look at the histograms (to get idea of the variance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['|img_mean|','|img_std|', '|img_std2|', '|img_kurtosis|', '|img_skewness|','|img_mode|'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assess feature imporance (using extra trees classifier)\n",
    "\n",
    "(sort of random forest, an ensemble method that will create classifiers based on random subsets) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First vectorize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into values\n",
    "le = LabelEncoder()\n",
    "df[\"|class|\"] = le.fit_transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into X Y vectors:\n",
    "feature_cols = ['|img_std|', '|img_mean|','|img_skewness|', '|img_mode|', '|img_kurtosis|', '|img_std2|']\n",
    "X = df.loc[:,feature_cols]\n",
    "y = df.loc[:,'|class|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then generate the classifier 'extra tries' and extract the importances\n",
    "(a la Pierluggi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=500,random_state=0)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extracting feature importance:\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d '%s' (%f)\" % (f + 1, indices[f], feature_cols[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, I am surprised that skewness scores higher than kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect the important features\n",
    "\n",
    "Let's make some plots based on the first 3 most important ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 - The 3 Most Important Features\",fontsize=14)\n",
    "ax = fig0.add_subplot(111, projection='3d')\n",
    "scatter_3d(ax, df, '|img_std|', '|img_std2|', '|img_skewness|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed with these three variables they are clearly separatable.\n",
    "\n",
    "But I think with the kurtosis or some of the others it would also work, as long as std and std2 are in there. So let's compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 - Combinations of important features\",fontsize=14)\n",
    "ax = fig0.add_subplot(221, projection='3d')\n",
    "scatter_3d(ax, df, '|img_std|', '|img_std2|', '|img_skewness|', color)\n",
    "ax = fig0.add_subplot(222, projection='3d')\n",
    "scatter_3d(ax, df, '|img_std|', '|img_std2|', '|img_kurtosis|', color)\n",
    "ax = fig0.add_subplot(224, projection='3d')\n",
    "scatter_3d(ax, df, '|img_std|', '|img_std2|', '|img_mean|', color)\n",
    "ax = fig0.add_subplot(223, projection='3d')\n",
    "scatter_3d(ax, df, '|img_std|', '|img_std2|', '|img_mode|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the std and std are leading, but you need a 3rd dimension for separation. Looking at the importances this is maybe not so surprising, as skewness, kurtosis, mode and mean are rather close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing result with Dimensionality Reduction Techniques\n",
    "\n",
    "(this has been analyzed in realxtals1-dimensionality1. To keep this notebook self-contained and independend, we do the dimensionality reduction here instead of exporting from the other notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, decomposition, datasets, random_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First assess PCA and IsoMap with 3 components\n",
    "(looked most promising in dimensionality assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create graph\n",
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 PCA & IsoMap (3 components)\",fontsize=14)\n",
    "\n",
    "# PCA (or SVD, which is almost the same)\n",
    "title = 'PCA'\n",
    "fieldnames = ['pca_1','pca_2','pca_3']\n",
    "pca = decomposition.TruncatedSVD(n_components=3)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(221, projection='3d', title=title)\n",
    "scatter_3d(ax, df_pca, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "                                                \n",
    "# Iso Map\n",
    "title = 'ISO'\n",
    "fieldnames = ['iso_1','iso_2','iso_3']\n",
    "iso = manifold.Isomap(n_neighbors=10, n_components=3)\n",
    "X_fit = iso.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(222, projection='3d', title=title)\n",
    "scatter_3d(ax, df_pca, fieldnames[0],fieldnames[1],fieldnames[2], color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing with range of dimensionality reduction techniques\n",
    "Let's assess more methods just as in the 'dimensionality notebook', but using three components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 Range of Manifold Learning Techniques (3 components)\",fontsize=14)\n",
    "\n",
    "fieldnames = ['comp_1','comp_2','comp_3']\n",
    "\n",
    "# PCA (or SVD, which is almost the same)\n",
    "title = 'PCA'\n",
    "pca = decomposition.TruncatedSVD(n_components=3)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(321, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "                                                                                              \n",
    "# Iso Map\n",
    "title = 'ISO'\n",
    "iso = manifold.Isomap(n_neighbors=10, n_components=3)\n",
    "X_fit = iso.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(322, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "\n",
    "# MDS\n",
    "title = 'MDS'\n",
    "mds = manifold.MDS(n_components=3, max_iter=100, n_init=1)\n",
    "X_fit = mds.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(323, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "\n",
    "# Spectral embedding\n",
    "title = 'Spectral Embedding'\n",
    "se = manifold.SpectralEmbedding(n_components=3, n_neighbors=10)\n",
    "X_fit = se.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(324, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "\n",
    "# Random Projection\n",
    "title = 'Random Projection'\n",
    "rp = random_projection.SparseRandomProjection(n_components=3, random_state=42)\n",
    "X_fit = rp.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(325, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "\n",
    "# t-SNE\n",
    "title = 't-SNE'\n",
    "tsne = manifold.TSNE(n_components=3, init='pca', random_state=42)\n",
    "X_fit = tsne.fit_transform(X)\n",
    "df_fit = pd.DataFrame(X_fit[:,0:3], columns=fieldnames)\n",
    "ax = fig0.add_subplot(326, projection='3d', title=title)\n",
    "scatter_3d(ax, df_fit, fieldnames[0],fieldnames[1],fieldnames[2], color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. More analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "* From visually inspecting these graphs and comparing those to the 'feature selection method', the 'feature selection' looks more separatable.\n",
    "* However, the feature selection method requires labeled data in order to train the classifier, while the manifest learning methods does not require labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps:\n",
    "- Try unsupervised learning on this data set (maybe first apply a manifold learning technique to optimize the data?) \n",
    "- Repeat this notebook on harder dataset\n",
    "\n",
    "\n",
    "Michael Janus, 17 July 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
