{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline (on Tileset7) - Aug 2017\n",
    "Created:  21 Aug 2018 <br>\n",
    "Last update: 29 Aug 2018\n",
    "\n",
    "\n",
    "### Goal: Combine the relevant steps from data import to unsupervised learning \n",
    "\n",
    "Many functions have gradually been developed in the prior notebooks (and added to 'imgutils'). In this notebook, the steps will be combined without all the intermediate analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2. Data Definitions & Feature Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data:\n",
    "datafolder = '../data/Crystals_Apr_12/Tileset6_subset_1K'\n",
    "n_tiles_x = 3  # mostly for visualization\n",
    "n_tiles_y = 3\n",
    "\n",
    "\n",
    "# Features to use:\n",
    "#feature_funcs = [imgutils.img_mean, imgutils.img_std, imgutils.img_median, \n",
    "#                 imgutils.img_mode,\n",
    "#                 imgutils.img_kurtosis, imgutils.img_skewness]\n",
    "feature_funcs = [imgutils.img_std, imgutils.img_relstd, imgutils.img_mean, \n",
    "                 imgutils.img_skewness,  imgutils.img_kurtosis, imgutils.img_mode, imgutils.img_range]\n",
    "feature_names = imgutils.stat_names(feature_funcs)\n",
    "\n",
    "# Size of the grid, specified as number of slices per image in x and y direction:\n",
    "default_grid_x = 4\n",
    "default_grid_y = default_grid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 3. Import Data & Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image import:\n",
    "print(\"Scanning for images in '{}'...\".format(datafolder))\n",
    "df_imgfiles = imgutils.scanimgdir(datafolder, '.tif')\n",
    "imgfiles = list(df_imgfiles['filename'])\n",
    "print(\"# of images: {} \\n\".format(len(imgfiles)))\n",
    "\n",
    "# feature extraction:\n",
    "print(\"Feature extraction...\")\n",
    "print(\"- Slicing up images in {} x {} patches. \".format(default_grid_y, default_grid_x))\n",
    "print(\"- Extract statistics from each slice: {} \".format(', '.join(feature_names)))\n",
    "print(\"...working...\", end='\\r')\n",
    "df = imgutils.slicestats(imgfiles, default_grid_y, default_grid_x, feature_funcs)\n",
    "print(\"# slices extracted: \", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 4. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyper-parameters\n",
    "default_n_clusters = 3\n",
    "\n",
    "# algorithm hyper-parameters:\n",
    "kmeans_n_init = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipeline2(X, ml_name, ml_algorithm, standardize=True, use_pca=True, n_pca=None):\n",
    "  \n",
    "    # Setup algorithmic pipeline, including standardization\n",
    "    pipeline = Pipeline([(ml_name, ml_algorithm)])\n",
    "    \n",
    "    # watch the order, pca should happen after scaling, but we insert at 0\n",
    "    if (use_pca): \n",
    "        pipeline.steps.insert(0,('pca', PCA(n_components=n_pca)))\n",
    "    if (standardize): \n",
    "        pipeline.steps.insert(0, ('scaling_{0}'.format(ml_name), StandardScaler()))\n",
    "    \n",
    "    # run the pipelines\n",
    "    y = pipeline.fit_predict(X) # calls predict on last step to get the labels\n",
    "\n",
    "    # report score:\n",
    "    score = silhouette_score(X, y)\n",
    "    \n",
    "    return score, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipelines2(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca=None):\n",
    "    global kmeans_n_init\n",
    "    \n",
    "    X = df_data.loc[:,feature_cols]\n",
    "    \n",
    "    # Setup ML clustering algorithms:    \n",
    "    kmeans = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=kmeans_n_init, init='k-means++')\n",
    "    agglomerative =  AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    # run the pipelines\n",
    "    print(\"Executing clustering pipelines...\")\n",
    "    score_kmeans, y_kmeans = run_ml_pipeline2(X, 'kmeans', kmeans, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    score_hier, y_hier = run_ml_pipeline2(X, 'hierarchical', agglomerative, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    # collect data\n",
    "    df_data['kmeans']=y_kmeans\n",
    "    df_data['hierarchical']=y_hier\n",
    "\n",
    "    # report results:\n",
    "    print(\"\\nClustering Scores:\")\n",
    "    print(\"K-means: \", score_kmeans)\n",
    "    print(\"Hierarchical: \", score_hier)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_ml_pipelines2(df, feature_names, default_n_clusters, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dummy'] = 0\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfile1 = imgfiles[0]\n",
    "#df_heats1 = df[df['filename']==imgfile1]['kmeans']\n",
    "#heats = np.reshape(df_heats1.values, (default_grid_y, default_grid_x))\n",
    "\n",
    "subimgs, heats = imgutils.getimgslices_fromdf(df, imgfile1, 'kmeans')\n",
    "heats = heats / np.max(heats)\n",
    "imgutils.showheatmap(subimgs, heats, cmapname='Set1', heatdepend_opacity = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches = 40\n",
    "df2 = imgutils.slicestats([imgfile1], n_patches, n_patches, feature_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines2(df2, ['img_std', 'img_range'], 3, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df2, 'kmeans', [imgfile1], n_rows=1, n_cols=1, fig_size=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(default_n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do these scores deviate from previous notebook?  Is there something going on with the sklearn pipeline or StandardScaler?\n",
    "\n",
    "\n",
    "<hr>\n",
    "## 5. Investigated scoring issue in other notebook\n",
    "\n",
    "(see realxtals1-pipeline_scoring_issues)\n",
    "\n",
    "Nothing was wrong with the pipeline or original step-by-step implementation. It was caused by different basis for the score calculation (pipeline impl. is using the original data while the step-by-step is using the normalized data for looking at the 'internal clustering score').\n",
    "\n",
    "As such, turning normalization off (in the pipeline) gives higher scores. This does not mean the outcome is better, that needs visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the ml_pipeline to use silhouette scoring based on it's last transformation:\n",
    "(later renamed the other ones to run_xxx2 to preserve them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "def run_ml_pipeline(X, ml_name, ml_algorithm, standardize=True, use_pca=True, n_pca=None):\n",
    "  \n",
    "    # Setup 'manual' pipeline (not using sklearn pipeline as intermediates are needed)\n",
    "    feat_data = X\n",
    "    if (standardize): \n",
    "        standardizer = StandardScaler()\n",
    "        X_norm = standardizer.fit_transform(X)     \n",
    "        feat_data = X_norm\n",
    "    if (use_pca):  \n",
    "        pca = PCA(n_components=n_pca)\n",
    "        X_pca = pca.fit_transform(feat_data)\n",
    "        feat_data = X_pca\n",
    "    \n",
    "    # run the pipelines\n",
    "    y = ml_algorithm.fit_predict(feat_data) # calls predict oto get the labels\n",
    "\n",
    "    # report score:\n",
    "    #score = silhouette_score(feat_data, y)\n",
    "    score = calinski_harabaz_score(feat_data,y)\n",
    "    \n",
    "    return score, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipelines(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca=None):\n",
    "    global kmeans_n_init\n",
    "    \n",
    "    X = df_data.loc[:,feature_cols]\n",
    "    \n",
    "    # Setup ML clustering algorithms:    \n",
    "    kmeans = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=kmeans_n_init, init='k-means++')\n",
    "    agglomerative =  AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    # run the pipelines\n",
    "    print(\"Executing clustering pipelines...\")\n",
    "    score_kmeans, y_kmeans = run_ml_pipeline(X, 'kmeans', kmeans, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    score_hier, y_hier = run_ml_pipeline(X, 'hierarchical', agglomerative, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    # collect data\n",
    "    df_data['kmeans']=y_kmeans\n",
    "    df_data['hierarchical']=y_hier\n",
    "\n",
    "    # report results:\n",
    "    print(\"\\nClustering Scores:\")\n",
    "    print(\"K-means: \", score_kmeans)\n",
    "    print(\"Hierarchical: \", score_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, default_n_clusters, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More consistent with previous results and imo it is indeed better to assess the algorithm on how good it could cluster after all pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, default_n_clusters, standardize=True, use_pca=True)\n",
    "s = (8,6)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it again without PCA and/pr normalization compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, default_n_clusters, standardize=True, use_pca=False)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, default_n_clusters, standardize=False, use_pca=False)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On this dataset, hierarchical works slightly better without normalization (!?!)\n",
    "\n",
    "Hence, the method that runs two pipelines is less useful as they need different parameterization. So let's make two methods below that run the full pipeline including slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine import and pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(imagefolder):\n",
    "    df_imgfiles = imgutils.scanimgdir(imagefolder, '.tif')\n",
    "    return list(df_imgfiles['filename'])  \n",
    "\n",
    "def extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols):\n",
    "    df = imgutils.slicestats(imgfiles, n_grid_rows, n_grid_cols, feature_funcs)\n",
    "    feature_names = imgutils.stat_names(feature_funcs)\n",
    "    return df, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans_pipeline(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca= None):\n",
    "    global kmeans_n_init\n",
    "   \n",
    "    ml_name=\"kmeans\"\n",
    "    ml_algorithm = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=kmeans_n_init, init='k-means++')\n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_hierarchical_pipeline(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca=None):\n",
    "\n",
    "    ml_name=\"hierarchical\"\n",
    "    ml_algorithm =  AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fullpipeline(imagefolder, n_image_rows, n_image_cols, \n",
    "                     n_grid_rows, n_grid_cols, feature_funcs, n_clusters, fig_size=(8,6), return_df = False):\n",
    "    \"\"\"\n",
    "    Run the full pipeline from import to visualization.   \n",
    "    \"\"\" \n",
    "    print(\"Working...\\r\")\n",
    "    imgfiles = import_data(imagefolder)\n",
    "    df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols)\n",
    "    print(feature_names)\n",
    "    score_kmeans = run_kmeans_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True )\n",
    "    score_hier = run_hierarchical_pipeline(df, feature_names, n_clusters, standardize=False, use_pca=False)\n",
    "\n",
    "    print('Results:')\n",
    "    print('Score k-means:', score_kmeans)\n",
    "    print('Score hierarchical:', score_hier)\n",
    "    \n",
    "    print('Visualizing...')\n",
    "    imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    \n",
    "    if return_df: return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try it out with different combinations of slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4x4 - 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6x6, 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8x8, 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 8. Try it out with different number of clusters\n",
    "\n",
    "### 2 clusters (4x4 , 6x6, 8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look again at hierarchical with scaling and pca on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles = import_data(datafolder)\n",
    "df_temp, feature_names = extract_features(imgfiles, feature_funcs, 6, 6)\n",
    "score = run_hierarchical_pipeline(df_temp, feature_names, 2, standardize=True, use_pca=True)\n",
    "print(\"With scaling & pca:\", score)\n",
    "imgutils.show_large_heatmap(df_temp, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)    \n",
    "score = run_hierarchical_pipeline(df_temp, feature_names, 2, standardize=False, use_pca=False)\n",
    "print(\"Without scaling & pca:\", score)\n",
    "imgutils.show_large_heatmap(df_temp, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " imgfiles = import_data(datafolder)\n",
    "df_temp, feature_names = extract_features(imgfiles, feature_funcs, 6, 6)\n",
    "score = run_kmeans_pipeline(df_temp, feature_names, 2, standardize=True, use_pca=True)\n",
    "print(\"With scaling & pca:\", score)\n",
    "imgutils.show_large_heatmap(df_temp, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)    \n",
    "score = run_kmeans_pipeline(df_temp, feature_names, 2, standardize=False, use_pca=False)\n",
    "print(\"Without scaling & pca:\", score)\n",
    "imgutils.show_large_heatmap(df_temp, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with and without scaling / pca are pretty close actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 clusters (4x4, 6x6, 8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need 'grid search' versions to scan parameter space!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Grid) Search for hyper-parameter optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles = import_data(datafolder)\n",
    "df, feature_names = extract_features(imgfiles, feature_funcs, default_grid_y, default_grid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9A. Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kmeans_nclusters(df_imgstats, feature_names, n_clusters_start, n_clusters_end):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (except for visualization) for different number of clusters\n",
    "    and show graph with score vs number of clusters.\n",
    "    \"\"\"   \n",
    "    n_count = n_clusters_end + 1 - n_clusters_start                        \n",
    "    result = np.empty(shape=(0,2), dtype=float)\n",
    "                              \n",
    "    for n in range(n_clusters_start, n_clusters_end+1):\n",
    "        score = run_kmeans_pipeline(df, feature_names, n, standardize=True, use_pca=True )\n",
    "        result = np.append(result, np.array([[n, score]]), axis=0)\n",
    "    \n",
    "    plt.plot(result[:,0], result[:,1])\n",
    "    plt.title('K-means Clustering - Score vs. Number of Clusters ')\n",
    "    plt.xlabel('# Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hierarchical_nclusters(df_imgstats, feature_names, n_clusters_start, n_clusters_end):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (except for visualization) for different number of clusters\n",
    "    and show graph with score vs number of clusters.\n",
    "    \"\"\"  \n",
    "    n_count = n_clusters_end + 1 - n_clusters_start                        \n",
    "    result = np.empty(shape=(0,2), dtype=float)\n",
    "                              \n",
    "    for n in range(n_clusters_start, n_clusters_end+1):\n",
    "        score = run_hierarchical_pipeline(df, feature_names, n, standardize=False, use_pca=False )\n",
    "        result = np.append(result, np.array([[n, score]]), axis=0)\n",
    "    \n",
    "    plt.plot(result[:,0], result[:,1])\n",
    "    plt.title('Hierarchical Clustering - Score vs. Number of Clusters ')\n",
    "    plt.xlabel('# Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_kmeans_nclusters(df, feature_names, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_hierarchical_nclusters(df, feature_names, 2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9B. Number of features / PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kmeans_pcacomponents(df_imgstats, feature_names, n_clusters):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (except for visualization) for different number of PCA componenents \n",
    "    and show graph with the score vs number of components.\n",
    "    \"\"\"       \n",
    "    n_count = len(feature_names) - 1            \n",
    "    result = np.empty(shape=(0,2), dtype=float)                              \n",
    "    \n",
    "    for n in range(1, len(feature_names)):\n",
    "        score = run_kmeans_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True, n_pca = n )\n",
    "        result = np.append(result, np.array([[n, score]]), axis=0)                 \n",
    "                   \n",
    "    plt.plot(result[:,0], result[:,1])\n",
    "    plt.title('K-means Clustering - Score vs. Number of PCA components ')\n",
    "    plt.xlabel('N PCA Components')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hierarchical_pcacomponents(df_imgstats, feature_names, n_clusters):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (except for visualization) for different number of PCA componenents \n",
    "    and show graph with the score vs number of components.\n",
    "    \"\"\"             \n",
    "    n_count = len(feature_names) - 1            \n",
    "    result = np.empty(shape=(0,2), dtype=float)                              \n",
    "    \n",
    "    for n in range(1, len(feature_names)):\n",
    "        score = run_hierarchical_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True, n_pca = n )\n",
    "        result = np.append(result, np.array([[n, score]]), axis=0)                 \n",
    "                   \n",
    "    plt.plot(result[:,0], result[:,1])\n",
    "    plt.title('Hierarchical Clustering - Score vs. Number of PCA components ')\n",
    "    plt.xlabel('N PCA Components')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_kmeans_pcacomponents(df, feature_names, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. This graph says that a single component (after PCA transform) would already do a good job. Let's visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = run_kmeans_pipeline(df, feature_names, 3, standardize=True, use_pca=True, n_pca=1)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed not bad. Maybe not so surprise as in early EDA we saw that on this particular set the standard deviation is very informative (we used it for manual labelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'manual grid search' showed for 4 clusters the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_kmeans_pcacomponents(df, feature_names, 4)\n",
    "score = run_kmeans_pipeline(df, feature_names, 4, standardize=True, use_pca=True, n_pca=1)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually this looks very good! It properly grouped 'fully occupied grid cells' from 'partially occupied'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at hierarchical clustering (although scores indicated better results without PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_hierarchical_pcacomponents(df, feature_names, 2)\n",
    "optimize_hierarchical_pcacomponents(df, feature_names, 3)\n",
    "optimize_hierarchical_pcacomponents(df, feature_names, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't know what these graphs really say; as the score is assessing cluster cohesion based on the last transformation in the pre-processing, it can be expected that less components are more separated. Maybe need to look at the 'elbows' here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize for 4 clusters with 1 pca component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = run_hierarchical_pipeline(df, feature_names, 4, standardize=True, use_pca=True, n_pca=1)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looks like pretty good grouping into 4 clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (Grid) Search for number of patches (expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the image slicing and feature extraction are the computationally expensive parts, both algorithms will be run in the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10A. Optimize and plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def optimize_slices(imagefolder, feature_funcs, n_clusters, list_n_slices):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (without visualization) for different number of image slices \n",
    "    and show graph of score vs number of slices.    \n",
    "    Note that same number of slices is used in x and y direction.\n",
    "    Returns a list of tuples of form (dataframe, n_slices, score_kmeans, score_hierarchical)  \n",
    "    \"\"\" \n",
    "    imgfiles = import_data(imagefolder)\n",
    "    results = []  \n",
    "\n",
    "    print(\"Running slice optimization for {} clusters:\".format(n_clusters))\n",
    "    counter=0\n",
    "    for n_slices in list_n_slices:\n",
    "        progress = 100 * counter / len(list_n_slices)\n",
    "        sys.stdout.write(\"{:.1f} %\".format(progress) + \"\\r\")\n",
    "        \n",
    "        df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows=n_slices, n_grid_cols=n_slices)\n",
    "        score_kmeans = run_kmeans_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True )\n",
    "        score_hier = run_hierarchical_pipeline(df, feature_names, n_clusters, standardize=False, use_pca=False)\n",
    "        results.append((df, n_slices, score_kmeans, score_hier))\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    sys.stdout.write(\"Done.\")\n",
    "    #sys.stdout.flush()\n",
    "    \n",
    "    plotdata = list(zip(*results))\n",
    "\n",
    "    plt.plot(plotdata[1], plotdata[2], label=\"k-means (with PCA)\")\n",
    "    plt.plot(plotdata[1], plotdata[3], label=\"hierarchical\")\n",
    "    plt.title('Clustering - Score vs. Number of Image Slices')\n",
    "    plt.xlabel('Number of slices')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_opt_results_3clusters = optimize_slices(datafolder, feature_funcs, n_clusters=3, list_n_slices=[2,4,8,10,12,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_opt_results_4clusters = optimize_slices(datafolder, feature_funcs, n_clusters=4, list_n_slices=[2,4,8,10,12,16,20,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, optimum number of slices is around 10 for 3 clusters, and even higher number of patches for 4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it is more efficient to do a ** grid search ** for clusters vs patches, as cluster evaluation is cheap while patches is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def gridsearch_slices(imagefolder, feature_funcs, list_n_clusters, list_n_slices):\n",
    "    \"\"\"\n",
    "    Run the full pipeline (without visualization) for different number of image slices \n",
    "    and show graph of score vs number of slices.    \n",
    "    Note that same number of slices is used in x and y direction.\n",
    "    Returns a list of tuples of form (dataframe, n_slices, n_clusters, score_kmeans, score_hierarchical)  \n",
    "    \"\"\" \n",
    "    imgfiles = import_data(imagefolder)\n",
    "    results = []  \n",
    "\n",
    "    print(\"Running slice-#clusters grid-search...\")\n",
    "    counter=0\n",
    "    for n_slices in list_n_slices:\n",
    "        progress = 100 * counter / len(list_n_slices)\n",
    "        sys.stdout.write(\"{:.1f} %\".format(progress) + \"\\r\")\n",
    "        \n",
    "        df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows=n_slices, n_grid_cols=n_slices)\n",
    "        \n",
    "        for n_clust in list_n_clusters:\n",
    "            df2 = df.copy()   # each result should have its own dataframe otherwise results are overwritten\n",
    "            score_kmeans = run_kmeans_pipeline(df2, feature_names, n_clust, standardize=True, use_pca=True )\n",
    "            score_hier = run_hierarchical_pipeline(df2, feature_names, n_clust, standardize=False, use_pca=False)\n",
    "            results.append((df2, n_slices, n_clust, score_kmeans, score_hier))\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    sys.stdout.write(\"Done.\")\n",
    "    \n",
    "    for n_clust in list_n_clusters:  \n",
    "        results_for_cluster = [t for t in results if t[2]==n_clust]    \n",
    "        plotdata = list(zip(*results_for_cluster))\n",
    "\n",
    "        plt.plot(plotdata[1], plotdata[3], label=\"k-means (with PCA)\")\n",
    "        plt.plot(plotdata[1], plotdata[4], label=\"hierarchical\")\n",
    "        plt.title('{} Clusters - Score vs. Number of Image Slices'.format(n_clust))\n",
    "        plt.xlabel('Number of slices')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_grid_search = gridsearch_slices(datafolder, feature_funcs, list_n_clusters=[2,3,4,5,6], list_n_slices=[2,4,6,8,10,12,14,16,20,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I one time ran it up to 64 slices, but that take forever. k-means flattens and hierarchical goes drops drastically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10B. Look at some heatmaps for 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_slicegridsearch(results, num_clust, num_slices):\n",
    "    results_for_cluster = [t for t in results if t[2]==num_clust]    \n",
    "    item_in_list = [t for t in results_for_cluster if t[1]==num_slices][0]\n",
    "    return item_in_list[0] \n",
    "\n",
    "def show_from_slicegridsearch(results, num_clust, num_slices, n_img_rows, n_img_cols, heat_col='kmeans', fig_size=(12,10)):\n",
    "    df_toshow = get_df_from_slicegridsearch(results, num_clust, num_slices)\n",
    "    imgfiles = df_toshow['filename'].unique().tolist()\n",
    "    extratitle = \"{}x{} slices, {} clusters\".format(num_slices,num_slices, num_clust)\n",
    "    imgutils.show_large_heatmap(df_toshow, heat_col, imgfiles, n_rows=n_img_rows, n_cols=n_img_cols, fig_size=fig_size, subtitle=extratitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_multiple_from_slicegridsearch(results, list_num_clust, list_num_slices, n_img_rows, n_img_cols, \n",
    "                                       heat_col='kmeans', fig_size=(12,10)):\n",
    "    for n_slices in list_num_slices:\n",
    "        for n_clust in list_num_clust:\n",
    "            show_from_slicegridsearch(results, n_clust, n_slices, n_img_rows, n_img_cols, heat_col,  fig_size=fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [2], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [2], [2,4,8, 10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='hierarchical',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10c. Look at some heatmaps for3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [3], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [3], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='hierarchical',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 10D. Look at some heatmaps for 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [4], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [4], [10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(16,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [4], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='hierarchical',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10D. Look at some heatmaps for 5 & 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [5], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_from_slicegridsearch(slice_grid_search, [6], [2, 4, 8 ,10], n_img_rows=n_tiles_y, n_img_cols=n_tiles_x, heat_col='kmeans',  fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Observations & Conclusions\n",
    "* developed 'full pipeline' functionality\n",
    "* looked at hyper-parameter optimization\n",
    "* k-means works really well for 2 clusters\n",
    "* for 4 clusters, hierarchical is better in sub-grouping the filled and partial filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.  Next steps:\n",
    "* move some of the methods here into (new) module for re-use\n",
    "* also add 'similarity learning' (see unsupervised2 notebook)\n",
    "* apply the 'full pipeline' on the harder data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael Janus, 29 August 2018 (1:50 am !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try two-step pipeline\n",
    "## step 1: filter out black tiles\n",
    "## step 2: cluster remaining tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_step1 = 3\n",
    "n_clusters_step2_kmeans = 3\n",
    "n_clusters_step2_hierarchical = 3\n",
    "\n",
    "n_patches_x = 16\n",
    "n_patches_y = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset\n",
    "df = df.drop(columns=['kmeans'])\n",
    "df2 = None\n",
    "df3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles = import_data(datafolder)\n",
    "df, feature_names = extract_features(imgfiles, feature_funcs, n_patches_y, n_patches_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: filter-out black tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_kmeans_pipeline(df, feature_names, n_clusters_step1, standardize=True, use_pca=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_select = 1  \n",
    "# we know for this it's the biggest set\n",
    "i_max_count = df['kmeans'].value_counts()\n",
    "cat_select = i_max_count.index[0]\n",
    "print(cat_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['kmeans']==cat_select]\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_kmeans = run_kmeans_pipeline(df2, feature_names, n_clusters_step2_kmeans, standardize=True, use_pca=True )\n",
    "score_hierarch = run_hierarchical_pipeline(df2, feature_names, n_clusters_step2_hierarchical, standardize=False, use_pca=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['kmeans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['hierarchical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns = {'kmeans':'kmeans2', 'hierarchical':'hierarchical2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.merge(df2, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['kmeans2'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['hierarchical2'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['heats']=df3['kmeans2']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['heats'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['heats2']=df3['hierarchical2']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['heats2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'heats', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(16,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'heats2', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(16,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANother subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder2 = '../data/Crystals_Apr_12/Tileset6_subset2_1K'\n",
    "n_tiles2_x = 2  # mostly for visualization\n",
    "n_tiles2_y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_fullpipeline(datafolder2, n_tiles2_y, n_tiles2_x, 10, 10, feature_funcs, 2, fig_size=(16,12), return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dummy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles = df['filename'].unique()\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgfiles, n_rows=n_tiles2_y, n_cols=n_tiles2_x, fig_size=(16,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
