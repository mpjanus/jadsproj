{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline (on Tileset7) - Aug 2017\n",
    "Created:  21 Aug 2018 <br>\n",
    "Last update: 24 Aug 2018\n",
    "\n",
    "\n",
    "### Goal: Combine the relevant steps from data import to unsupervised learning \n",
    "\n",
    "Many functions have gradually been developed in the prior notebooks (and added to 'imgutils'). In this notebook, the steps will be combined without all the intermediate analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2. Data Definitions & Feature Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data:\n",
    "datafolder = '../data/Crystals_Apr_12/Tileset7'\n",
    "n_tiles_x = 3  # mostly for visualization\n",
    "n_tiles_y = 2\n",
    "\n",
    "\n",
    "# Features to use:\n",
    "#feature_funcs = [imgutils.img_mean, imgutils.img_std, imgutils.img_median, \n",
    "#                 imgutils.img_mode,\n",
    "#                 imgutils.img_kurtosis, imgutils.img_skewness]\n",
    "feature_funcs = [imgutils.img_std, imgutils.img_relstd, imgutils.img_mean, \n",
    "                 imgutils.img_skewness,  imgutils.img_kurtosis, imgutils.img_mode]\n",
    "feature_names = imgutils.stat_names(feature_funcs)\n",
    "\n",
    "# Size of the grid, specified as number of slices per image in x and y direction:\n",
    "n_rows = 4\n",
    "n_cols = n_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 3. Import Data & Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image import:\n",
    "print(\"Scanning for images in '{}'...\".format(datafolder))\n",
    "df_imgfiles = imgutils.scanimgdir(datafolder, '.tif')\n",
    "imgfiles = list(df_imgfiles['filename'])\n",
    "print(\"# of images: {} \\n\".format(len(imgfiles)))\n",
    "\n",
    "# feature extraction:\n",
    "print(\"Feature extraction...\")\n",
    "print(\"- Slicing up images in {} x {} patches. \".format(n_rows, n_cols))\n",
    "print(\"- Extract statistics from each slice: {} \".format(', '.join(feature_names)))\n",
    "print(\"...working...\", end='\\r')\n",
    "df = imgutils.slicestats(imgfiles, n_rows, n_cols, feature_funcs)\n",
    "print(\"# slices extracted: \", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 4. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyper-parameters\n",
    "n_clusters = 3\n",
    "n_important_features = len(feature_names)\n",
    "\n",
    "# algorithm hyper-parameters:\n",
    "kmeans_n_init = 10\n",
    "pca_n_components = None   # i.e. all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipeline(X, ml_name, ml_algorithm, standardize=True, use_pca=True):\n",
    "    global pca_n_components\n",
    "  \n",
    "    # Setup algorithmic pipeline, including standardization\n",
    "    pipeline = Pipeline([(ml_name, ml_algorithm)])\n",
    "    \n",
    "    # watch the order, pca should happen after scaling, but we insert at 0\n",
    "    if (use_pca): \n",
    "        pipeline.steps.insert(0,('pca', PCA(n_components=pca_n_components)))\n",
    "    if (standardize): \n",
    "        pipeline.steps.insert(0, ('scaling_{0}'.format(ml_name), StandardScaler()))\n",
    "    \n",
    "    # run the pipelines\n",
    "    y = pipeline.fit_predict(X) # calls predict on last step to get the labels\n",
    "\n",
    "    # report score:\n",
    "    score = silhouette_score(X, y)\n",
    "    \n",
    "    return score, y\n",
    "\n",
    "def run_ml_pipelines(df_data, feature_cols, n_clust = n_clusters, standardize=True, use_pca=True):\n",
    "    global pca_n_components, kmeans_n_init\n",
    "    \n",
    "    X = df_data.loc[:,feature_cols]\n",
    "    \n",
    "    # Setup ML clustering algorithms:    \n",
    "    kmeans = KMeans(algorithm='auto', n_clusters=n_clust, n_init=kmeans_n_init, init='k-means++')\n",
    "    agglomerative =  AgglomerativeClustering(n_clusters=n_clust, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    # run the pipelines\n",
    "    print(\"Executing clustering pipelines...\")\n",
    "    score_kmeans, y_kmeans = run_ml_pipeline(X, 'kmeans', kmeans, standardize = standardize, use_pca = use_pca)\n",
    "    score_hier, y_hier = run_ml_pipeline(X, 'hierarchical', agglomerative, standardize = standardize, use_pca = use_pca)\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    # collect data\n",
    "    df_data['kmeans']=y_kmeans\n",
    "    df_data['hierarchical']=y_hier\n",
    "\n",
    "    # report results:\n",
    "    print(\"\\nClustering Scores:\")\n",
    "    print(\"K-means: \", score_kmeans)\n",
    "    print(\"Hierarchical: \", score_hier)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do these scores deviate from previous notebook?  Is there something going on with the sklearn pipeline or StandardScaler?\n",
    "\n",
    "\n",
    "<hr>\n",
    "## 5. Investigated scoring issue in other notebook\n",
    "\n",
    "(see realxtals1-pipeline_scoring_issues)\n",
    "\n",
    "Nothing was wrong with the pipeline or original step-by-step implementation. It was caused by different basis for the score calculation (pipeline impl. is using the original data while the step-by-step is using the normalized data for looking at the 'internal clustering score').\n",
    "\n",
    "As such, turning normalization off (in the pipeline) gives higher scores. This does not mean the outcome is better, that needs visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (8,6)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it again without normalization and PCA and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, standardize=False, use_pca=False)\n",
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)\n",
    "imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On this dataset, hierarchical works slightly better without normalization (!?!)\n",
    "\n",
    "Hence, the method that runs two pipelines is less useful as they need different parameterization. So let's make two methods below that run the full pipeline including slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine import and pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(imagefolder):\n",
    "    df_imgfiles = imgutils.scanimgdir(datafolder, '.tif')\n",
    "    return list(df_imgfiles['filename'])  \n",
    "\n",
    "def extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols):\n",
    "    df = imgutils.slicestats(imgfiles, n_grid_rows, n_grid_cols, feature_funcs)\n",
    "    feature_names = imgutils.stat_names(feature_funcs)\n",
    "    return df, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans_pipeline(df_data, feature_cols, n_clust = n_clusters, standardize=True, use_pca=True):\n",
    "    global kmeans_n_init\n",
    "   \n",
    "    ml_name=\"kmeans\"\n",
    "    ml_algorithm = KMeans(algorithm='auto', n_clusters=n_clust, n_init=kmeans_n_init, init='k-means++')\n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_hierarchical_pipeline(df_data, feature_cols, n_clust = n_clusters, standardize=True, use_pca=True):\n",
    "\n",
    "    ml_name=\"hierarchical\"\n",
    "    ml_algorithm =  AgglomerativeClustering(n_clusters=n_clust, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fullpipeline(imagefolder, n_image_rows, n_image_cols, \n",
    "                     n_grid_rows, n_grid_cols, feature_funcs, n_clusters):\n",
    "    \"\"\"\n",
    "    Run the full pipeline from import to visualization.   \n",
    "    \"\"\" \n",
    "    print(\"Working...\\r\")\n",
    "    imgfiles = import_data(imagefolder)\n",
    "    df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols)\n",
    "    score_kmeans = run_kmeans_pipeline(df, feature_names, n_clust=n_clusters, standardize=True, use_pca=True )\n",
    "    score_hier = run_hierarchical_pipeline(df, feature_names, n_clust=n_clusters, standardize=False, use_pca=False)\n",
    "\n",
    "    print('Results:')\n",
    "    print('Score k-means:', score_kmeans)\n",
    "    print('Score hierarchical:', score_hier)\n",
    "    \n",
    "    print('Visualizing...')\n",
    "    s = (8,6)\n",
    "    imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=s)\n",
    "    imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try it out with different combinations of slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4x4 - 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6x6, 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8x8, 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 8. Try it out with different number of clusters\n",
    "\n",
    "### 2 clusters (4x4 , 6x6, 8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 clusters (4x4, 6x6, 8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 4, 4, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 6, 6, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fullpipeline(datafolder, n_tiles_y, n_tiles_x, 8, 8, feature_funcs, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need grid search versions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
