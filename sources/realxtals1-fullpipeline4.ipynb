{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline on Polymers (very very hard) - Nov 2018\n",
    "Created:  04 Nov 2018 <br>\n",
    "Last update: 04 Nov 2018\n",
    "\n",
    "\n",
    "### Goal: Run the full pipeline on the Polymer sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def change_clusternums(df, columnname, oldnew_dict):\n",
    "    df[columnname].replace(oldnew_dict, inplace=True)\n",
    "    \n",
    "def swap_clusters(df, columnname, clust1, clust2):\n",
    "    oldnew_dict = { clust1: clust2, clust2: clust1}\n",
    "    df[columnname].replace(oldnew_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2. Data Definitions & Feature Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data:\n",
    "datafolder = '../data/Polymers_27Sep2018/LowMag_1K_Subset'\n",
    "n_tiles_x = 3  # mostly for visualization\n",
    "n_tiles_y = 2\n",
    "\n",
    "\n",
    "# Features to use:\n",
    "#feature_funcs = [imgutils.img_mean, imgutils.img_std, imgutils.img_median, \n",
    "#                 imgutils.img_mode, imgutils.img_kurtosis, imgutils.img_skewness]\n",
    "feature_funcs = [imgutils.img_std, imgutils.img_relstd, imgutils.img_mean, \n",
    "                 imgutils.img_skewness,  imgutils.img_kurtosis, imgutils.img_mode, imgutils.img_range]\n",
    "feature_names = imgutils.stat_names(feature_funcs)\n",
    "\n",
    "# Size of the grid, specified as number of slices per image in x and y direction:\n",
    "default_grid_x = 8\n",
    "default_grid_y = default_grid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 3. Import Data & Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image import:\n",
    "print(\"Scanning for images in '{}'...\".format(datafolder))\n",
    "df_imgfiles = imgutils.scanimgdir(datafolder, '.tif')\n",
    "imgfiles = list(df_imgfiles['filename'])\n",
    "print(\"# of images: {} \\n\".format(len(imgfiles)))\n",
    "\n",
    "# feature extraction:\n",
    "print(\"Feature extraction...\")\n",
    "print(\"- Slicing up images in {} x {} patches. \".format(default_grid_y, default_grid_x))\n",
    "print(\"- Extract statistics from each slice: {} \".format(', '.join(feature_names)))\n",
    "print(\"...working...\", end='\\r')\n",
    "df = imgutils.slicestats(imgfiles, default_grid_y, default_grid_x, feature_funcs)\n",
    "print(\"# slices extracted: \", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a set of 2 x 3 images covering a larger area (a so called 'tile set')\n",
    "imgutils.showimgset(imgfiles, n_tiles_y, n_tiles_x, fig_size=(12, 8), relspacing=(0.05,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 4. Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data hyper-parameters\n",
    "default_n_clusters = 3\n",
    "\n",
    "# algorithm hyper-parameters:\n",
    "kmeans_n_init = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ML pipeline **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipeline(X, ml_name, ml_algorithm, standardize=True, use_pca=True, n_pca=None):\n",
    "  \n",
    "    # Setup 'manual' pipeline (not using sklearn pipeline as intermediates are needed)\n",
    "    feat_data = X\n",
    "    if (standardize): \n",
    "        standardizer = StandardScaler()\n",
    "        X_norm = standardizer.fit_transform(X)     \n",
    "        feat_data = X_norm\n",
    "    if (use_pca):  \n",
    "        pca = PCA(n_components=n_pca)\n",
    "        X_pca = pca.fit_transform(feat_data)\n",
    "        feat_data = X_pca\n",
    "    \n",
    "    # run the pipelines\n",
    "    y = ml_algorithm.fit_predict(feat_data) # calls predict oto get the labels\n",
    "\n",
    "    # report score:\n",
    "    score = silhouette_score(feat_data, y)\n",
    "    \n",
    "    return score, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipelines(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca=None):\n",
    "    global kmeans_n_init\n",
    "    \n",
    "    X = df_data.loc[:,feature_cols]\n",
    "    \n",
    "    # Setup ML clustering algorithms:    \n",
    "    kmeans = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=kmeans_n_init, init='k-means++')\n",
    "    agglomerative =  AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    # run the pipelines\n",
    "    print(\"Executing clustering pipelines...\")\n",
    "    score_kmeans, y_kmeans = run_ml_pipeline(X, 'kmeans', kmeans, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    score_hier, y_hier = run_ml_pipeline(X, 'hierarchical', agglomerative, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    print(\"Done\\n\")\n",
    "    \n",
    "    # collect data\n",
    "    df_data['kmeans']=y_kmeans\n",
    "    df_data['hierarchical']=y_hier\n",
    "\n",
    "    # report results:\n",
    "    print(\"\\nClustering Scores:\")\n",
    "    print(\"K-means: \", score_kmeans)\n",
    "    print(\"Hierarchical: \", score_hier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Combine with import **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(imagefolder):\n",
    "    df_imgfiles = imgutils.scanimgdir(imagefolder, '.tif')\n",
    "    return list(df_imgfiles['filename'])  \n",
    "\n",
    "def extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols):\n",
    "    df = imgutils.slicestats(imgfiles, n_grid_rows, n_grid_cols, feature_funcs)\n",
    "    feature_names = imgutils.stat_names(feature_funcs)\n",
    "    return df, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans_pipeline(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca= None):\n",
    "    global kmeans_n_init\n",
    "   \n",
    "    ml_name=\"kmeans\"\n",
    "    ml_algorithm = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=kmeans_n_init, init='k-means++')\n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_hierarchical_pipeline(df_data, feature_cols, n_clusters, standardize=True, use_pca=True, n_pca=None):\n",
    "\n",
    "    ml_name=\"hierarchical\"\n",
    "    ml_algorithm =  AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')  \n",
    "\n",
    "    X = df_data.loc[:,feature_cols]    \n",
    "    score, y = run_ml_pipeline(X, ml_name, ml_algorithm, standardize = standardize, use_pca = use_pca, n_pca=n_pca)\n",
    "    df_data[ml_name]= y\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fullpipeline(imagefolder, n_image_rows, n_image_cols, \n",
    "                     n_grid_rows, n_grid_cols, feature_funcs, n_clusters, fig_size=(8,6), return_df = False):\n",
    "    \"\"\"\n",
    "    Run the full pipeline from import to visualization.   \n",
    "    \"\"\" \n",
    "    print(\"Working...\\r\")\n",
    "    imgfiles = import_data(imagefolder)\n",
    "    df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols)\n",
    "    print(feature_names)\n",
    "    score_kmeans = run_kmeans_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True )\n",
    "    score_hier = run_hierarchical_pipeline(df, feature_names, n_clusters, standardize=False, use_pca=False)\n",
    "\n",
    "    print('Results:')\n",
    "    print('Score k-means:', score_kmeans)\n",
    "    print('Score hierarchical:', score_hier)\n",
    "    \n",
    "    print('Visualizing...')\n",
    "    imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    \n",
    "    if return_df: return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fullpipeline_kmeans(imagefolder, n_image_rows, n_image_cols, \n",
    "                     n_grid_rows, n_grid_cols, feature_funcs, n_clusters, fig_size=(8,6), return_df = False):\n",
    "    \"\"\"\n",
    "    Run the full pipeline from import to visualization.   \n",
    "    \"\"\" \n",
    "    print(\"Working...\\r\")\n",
    "    imgfiles = import_data(imagefolder)\n",
    "    df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols)\n",
    "    print(feature_names)\n",
    "    score_kmeans = run_kmeans_pipeline(df, feature_names, n_clusters, standardize=True, use_pca=True )\n",
    "\n",
    "    print('Results:')\n",
    "    print('Score k-means:', score_kmeans)\n",
    "    \n",
    "    print('Visualizing...')\n",
    "    imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    \n",
    "    if return_df: return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fullpipeline_hierarchical(imagefolder, n_image_rows, n_image_cols, \n",
    "                     n_grid_rows, n_grid_cols, feature_funcs, n_clusters, fig_size=(8,6), return_df = False):\n",
    "    \"\"\"\n",
    "    Run the full pipeline from import to visualization.   \n",
    "    \"\"\" \n",
    "    print(\"Working...\\r\")\n",
    "    imgfiles = import_data(imagefolder)\n",
    "    df, feature_names = extract_features(imgfiles, feature_funcs, n_grid_rows, n_grid_cols)\n",
    "    print(feature_names)\n",
    "    score_hier = run_hierarchical_pipeline(df, feature_names, n_clusters, standardize=False, use_pca=False)\n",
    "\n",
    "    print('Results:')\n",
    "    print('Score hierarchical:', score_hier)\n",
    "    \n",
    "    print('Visualizing...')\n",
    "    imgutils.show_large_heatmap(df, 'hierarchical', imgfiles, n_rows=n_image_rows, n_cols=n_image_cols, fig_size=fig_size)\n",
    "    \n",
    "    if return_df: return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sliding window **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.image as skimgfeat\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def run_pipeline_windowed(imgfilename, patch_size, n_clusters, downscale_factor=2, return_cluster_image = False,\n",
    "                     algorithm='kmeans', show_results=True, show_diagnostics=False, show_diagnostics_extra=False):\n",
    "    \"\"\" \"\"\"\n",
    "    \n",
    "    print(\"Importing image(s)s...\")\n",
    "    img_full = imgutils.loadtiff(imgfilename)\n",
    "    img = imgutils.downsample_img(img_full, downscale_factor)    \n",
    "    patches = skimgfeat.extract_patches_2d(img, patch_size)\n",
    "\n",
    "    sys.stdout.write(\"Extracting features...\")\n",
    "    patchstats = np.empty((patches.shape[0],4))\n",
    "    n_progress_update = (int)(patches.shape[0] / 1000) \n",
    "    for i in range(patches.shape[0]):\n",
    "        patch = patches[i]\n",
    "        patchstats[i,0] = np.mean(patch)\n",
    "        patchstats[i,1] = np.median(patch)\n",
    "        patchstats[i,2] = np.std(patch)\n",
    "        patchstats[i,3] = np.max(patch)-np.min(patch) \n",
    "        if (i % n_progress_update == 0):\n",
    "            progress = (int)(i*100.0 / patches.shape[0])\n",
    "            sys.stdout.write(\"\\rExtracting features... {:d} %\".format(progress))\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\rExtracting features... 100 %\\n\")\n",
    "    sys.stdout.flush()\n",
    "        \n",
    "    print(\"Clustering into {} clusters...\".format(n_clusters))\n",
    "    kmeans = KMeans(algorithm='auto', n_clusters=n_clusters, n_init=10, init='k-means++')\n",
    "    hierarch = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')\n",
    "    if (algorithm=='kmeans'):\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), ('pca', PCA()), ('kmeans',kmeans)])\n",
    "    elif (algorithm=='hierarchical'):\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), ('pca', PCA()), ('hierarchical',hierarch)])\n",
    "    else:\n",
    "        raise ValueException(\"unsupported algorithm {}\".format(algorithm))\n",
    "        \n",
    "    #x = patchstats\n",
    "    x = patchstats[:,1:3]  # only mean, std and range\n",
    "    y = pipeline.fit_predict(x)\n",
    "\n",
    "    dim = (int)(math.sqrt(y.shape[0]))\n",
    "    img_clust = np.reshape(y, (dim, dim))  \n",
    "    \n",
    "    if show_results:\n",
    "        print(\"Visualizing results...\")          \n",
    "        plot_with_overlay(img, img_clust, title='cluster heatmap')\n",
    "        plt.show()\n",
    "        \n",
    "    if show_diagnostics: \n",
    "        print(\"Showing diagnostic images...\")      \n",
    "        plot_with_overlay(img, img_clust, show_overlay=False, title='original image')\n",
    "        plot_with_overlay(img, img_clust, show_org=False, title='local clusters')\n",
    "        plt.show()\n",
    "        \n",
    "    if show_diagnostics_extra:\n",
    "        print(\"Showing diagnostic feature images...\")   \n",
    "        img_mean = np.reshape(patchstats[:,0], (dim, dim))\n",
    "        img_median = np.reshape(patchstats[:,1], (dim, dim))\n",
    "        img_std = np.reshape(patchstats[:,2], (dim, dim))\n",
    "        img_range = np.reshape(patchstats[:,3], (dim, dim))\n",
    "        plot_with_overlay(img, img_mean, title='local mean')\n",
    "        plot_with_overlay(img, img_median, title='local median')\n",
    "        plot_with_overlay(img, img_std, title='local standard deviation')\n",
    "        plot_with_overlay(img, img_range, title='local range')\n",
    "        \n",
    "    if return_cluster_image:\n",
    "        return img, img_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_overlay(orgimg, overlayimg, fig_size=(10,10), show_org=True, show_overlay=True, \n",
    "                      overlay_alpha=0.3, cmapname='Spectral', title=None):\n",
    "    l = (orgimg.shape[0] - overlayimg.shape[0]) \n",
    "    t = (orgimg.shape[1] - overlayimg.shape[1])   \n",
    "    r = (orgimg.shape[0] - l)\n",
    "    b = (orgimg.shape[1] - t)\n",
    "           \n",
    "    cmin = 1.1*np.min(overlayimg)\n",
    "    cmax = 1.1*np.max(overlayimg)\n",
    "    _ = plt.figure(figsize=fig_size)\n",
    "    if show_org: \n",
    "        plt.imshow(orgimg, cmap='gray', origin='upper', extent=[0,orgimg.shape[0], 0, orgimg.shape[1]])\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        overlay_alpha=0.8\n",
    "    if (show_overlay):\n",
    "        plt.imshow(overlayimg, cmap=cmapname, alpha=overlay_alpha, vmin=cmin, vmax=cmax, origin='upper', extent=[l,r,t,b])\n",
    "        plt.axis('off')\n",
    "    if (title): plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_cluster_image(cluster_img, cluster_num):\n",
    "    return (cluster_img==cluster_num).astype(int) \n",
    "    \n",
    "def show_cluster_images(img, cluster_img, n_clusters, show_img=False, cmapname='tab10'):\n",
    "    for i in range(n_clusters):\n",
    "        img_clust_i = get_single_cluster_image(cluster_img, i) \n",
    "        plot_with_overlay(img, img_clust_i, title='cluster {}'.format(i), show_org=show_img, cmapname=cmapname)\n",
    "        \n",
    "def show_single_cluster_image(img, cluster_img, cluster_to_show, show_img=True, opacity=0.5, cmapname='RdYlGn'):\n",
    "    img_clust_i = get_single_cluster_image(cluster_img, cluster_to_show) \n",
    "    plot_with_overlay(img, img_clust_i, title='cluster {}'.format(cluster_to_show), show_org=show_img, overlay_alpha=opacity, cmapname=cmapname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches_y = 10\n",
    "n_patches_x = 10\n",
    "df = imgutils.slicestats(imgfiles, n_patches_y, n_patches_x, feature_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_ml_pipelines(df, feature_names, default_n_clusters, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['dummy'] = 0\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_clusters(df, 'kmeans', 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "** Try on single image with much smaller patch size **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_patches = 40\n",
    "df2 = imgutils.slicestats([imgfile1], n_patches, n_patches, feature_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_ml_pipelines(df2, feature_names, 6, standardize=True, use_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df2, 'kmeans', [imgfile1], n_rows=1, n_cols=1, fig_size=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This set requires the 2 step approach (i.e. first filter out the black tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the ml_pipeline to use silhouette scoring based on it's last transformation:\n",
    "(later renamed the other ones to run_xxx2 to preserve them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try two-step pipeline - first on single tile\n",
    "## step 1: filter out black tiles\n",
    "## step 2: cluster remaining tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_clusters_step1 = 2\n",
    "n_clusters_step2_kmeans = 3\n",
    "n_clusters_step2_hierarchical = 3\n",
    "\n",
    "n_patches_x = 40\n",
    "n_patches_y = 40\n",
    "\n",
    "n_tiles_x = 1\n",
    "n_tiles_y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reset\n",
    "#df = df.drop(columns=['kmeans'])\n",
    "df2 = None\n",
    "df3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgfiles = import_data(datafolder)\n",
    "imgfiles = [imgfiles[4]]\n",
    "df, feature_names = extract_features(imgfiles, feature_funcs, n_patches_y, n_patches_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['dummy']=0\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,12))\n",
    "imgutils.show_large_heatmap(df, 'dummy', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,12), no_borders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: filter-out black tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = run_kmeans_pipeline(df, feature_names, n_clusters_step1, standardize=True, use_pca=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_clusters(df,'kmeans', 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['kmeans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cat_select = 1  \n",
    "# we know for this it's the biggest set\n",
    "i_max_count = df['kmeans'].value_counts()\n",
    "cat_select = i_max_count.index[0]\n",
    "print(cat_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NO, it's cat 2!\n",
    "cat_select = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df[df['kmeans']==cat_select]\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_kmeans = run_kmeans_pipeline(df2, feature_names, n_clusters_step2_kmeans, standardize=True, use_pca=True )\n",
    "score_hierarch = run_hierarchical_pipeline(df2, feature_names, n_clusters_step2_hierarchical, standardize=False, use_pca=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['kmeans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2['hierarchical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2=df2.rename(columns = {'kmeans':'kmeans2', 'hierarchical':'hierarchical2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3 = df.merge(df2, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['kmeans2'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['hierarchical2'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['heats']=df3['kmeans2']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['heats'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make the whole 2 clusters only\n",
    "#df3['heats'].replace({1:0}, inplace=True)\n",
    "#df3['heats'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['heats2']=df3['hierarchical2']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['heats2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make the whole 2 clusters only\n",
    "#df3['heats2'].replace({1:0}, inplace=True)\n",
    "#df3['heats2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df3, 'heats', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(16,12))\n",
    "imgutils.show_large_heatmap(df3, 'heats', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=(16,12), heatdependent_opacity=True, no_borders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Try sanitized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder_s = '../data/Polymers_27Sep2018/LowMag_NoBlack'\n",
    "n_tiles_x_s = 3  # mostly for visualization\n",
    "n_tiles_y_s = 2\n",
    "\n",
    "n_patches_y_s=10\n",
    "n_patches_x_s=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_funcs2 = feature_funcs\n",
    "feature_names2 = imgutils.stat_names(feature_funcs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles_s = import_data(datafolder_s)\n",
    "imgfiles_s = imgfiles_s[:6]\n",
    "df_s, feature_names = extract_features(imgfiles_s, feature_funcs2, n_patches_y_s, n_patches_x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.showimgset(imgfiles_s, 2,3, fig_size=(12, 8), relspacing=(0.05,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s['dummy']=0\n",
    "imgutils.show_large_heatmap(df_s, 'dummy', imgfiles_s, n_rows=n_tiles_y_s, n_cols=n_tiles_x_s, fig_size=(12,12), no_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_kmeans_pipeline(df_s, feature_names2, 3, standardize=True, use_pca=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df_s, 'kmeans', imgfiles_s, n_rows=n_tiles_y_s, n_cols=n_tiles_x_s, fig_size=(12,12), opacity=0.2, no_borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_clusters(df_s, 'kmeans', 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgutils.show_large_heatmap(df_s, 'kmeans', imgfiles_s, n_rows=n_tiles_y_s, n_cols=n_tiles_x_s, fig_size=(12,12), opacity=0.2, no_borders=True, heatdependent_opacity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfilename = imgfiles_s[3]\n",
    "img = imgutils.loadtiff(imgfilename)\n",
    "imgutils.showimg(img, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, h = run_pipeline_windowed(imgfilename, (20,20), 2, return_cluster_image=True, downscale_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** and on another **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfilename = imgfiles_s[1]\n",
    "img = imgutils.loadtiff(imgfilename)\n",
    "imgutils.showimg(img, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, h = run_pipeline_windowed(imgfilename, (20,20), 2, return_cluster_image=True, downscale_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_overlay(img, h, title='cluster heatmap', cmapname='Purples', fig_size=(10,10), overlay_alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cluster_images(img, h, 2, show_img=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
