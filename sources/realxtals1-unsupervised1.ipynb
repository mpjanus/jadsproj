{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised Learning (on Tileset7) - July 2017\n",
    "Created:  24 July 2018 <br>\n",
    "Last update: 24 july 2018\n",
    "\n",
    "\n",
    "### Try a number of unsupervised learning techniques on a simple data set\n",
    "\n",
    "The data used here has been pre-labelled in one of my prior notebooks. The labeleing here functions as check, but should not be used for the learning itself (as the goal is to achieve the clustering in unsupervised fashion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove warnings messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "import imgutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell if you altered imgutils\n",
    "import importlib\n",
    "importlib.reload(imgutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 2. Import Crystal Image Data & Statistics\n",
    "The data was labeled and exported to csv in the notebook realxtals1_dataeng1.ipynb\n",
    "\n",
    "#### About the data:\n",
    "The CSV contains the image files, slice information (sub-images) and associated statistics, which are the features for which a classifier needs to be found. \n",
    "\n",
    "The goal is to find the clustering in feature-space and use those to categorize the images. For this particular dataset, a single statistics could be used to label into three classes:<br>\n",
    "\n",
    "A = subimage contains no crystal, <br>\n",
    "B = part of subimage contains crystal, <br>\n",
    "C = (most of) subimage contains crystal\n",
    "\n",
    "But the labels have been added here for analyses, eventually the data will be unlabelled.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Crystals_Apr_12/Tileset7-2.csv', sep=';')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 3. Quick visual inspection of the 'feature space'\n",
    "\n",
    "(Some of this is a repeat from the feature selection notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it in 3 dimensions, choosing some stat combinations\n",
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle(\"Tileset 7 - Exploring feature space\",fontsize=14)\n",
    "\n",
    "# trick to convert category labels into color codes\n",
    "color = pd.DataFrame(df['class'].astype('category'))['class'].cat.codes\n",
    "\n",
    "# define alias for later reference\n",
    "org_labels = color\n",
    "\n",
    "def scatter_3d(ax, df, feat1, feat2, feat3, colors):\n",
    "    ax.scatter(df[feat1], df[feat2], df[feat3], c=colors)\n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.set_zlabel(feat3)\n",
    "\n",
    "\n",
    "ax = fig0.add_subplot(221, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)\n",
    "\n",
    "ax = fig0.add_subplot(222, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_kurtosis|', '|img_skewness|', color)\n",
    "\n",
    "ax = fig0.add_subplot(223, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mode|', '|img_kurtosis|', '|img_std|', color)\n",
    "\n",
    "ax = fig0.add_subplot(224, projection='3d')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_mode|', '|img_std|', color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also make some box plots of the individual features\n",
    "(practising new skills learned from datacamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['|img_mean|','|img_std|', '|img_std2|', '|img_kurtosis|', '|img_skewness|','|img_mode|']]\n",
    "df_subset.plot(kind='box', subplots=True,figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All have many outliers, which could be related to the 'separation' of classes. I need **interactive box plots** that show the images!\n",
    "\n",
    "(add to TODO list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 4. Let's try k-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create numbers for classed for better plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"|class|\"] = le.fit_transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First vectorize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into X Y vectors:\n",
    "feature_cols = ['|img_std|', '|img_std2|', '|img_mean|','|img_skewness|', '|img_kurtosis|','|img_mode|']\n",
    "X = df.loc[:,feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = cluster.KMeans(algorithm='auto', n_clusters=3, n_init=10, init='k-means++')\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.labels_)\n",
    "print(k_means.cluster_centers_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eh... this is multidimensional space. Lets' see if there is a way to visualize this in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting first three dimensions (i.e. std, std2 and mean) the k-means and the original labels\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Unsupervised K-Means \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='K Means')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', k_means.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect (some mistakes), but not that bad! And that for a first attempt.\n",
    "\n",
    "Is there a difference between the assigned label and using predict on the trainng data? Let's check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into one dataframe:\n",
    "df2 = pd.concat([df, pd.Series(k_means.labels_)], axis=1)\n",
    "df2 = df2.rename(columns = { 0 : 'k_means'})\n",
    "df2['k_means_predict'] = k_means.predict(X)\n",
    "\n",
    "# plotting first three dimensions (i.e. std, std2 and mean) the k-means and the original labels\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Unsupervised K-Means - Labels vs Predict\",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='K Means')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', colors=df2['k_means'])\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', colors=df2['k_means_predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it's the same (pfew!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 5. Try DBScan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = cluster.DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting first three dimensions (i.e. std, std2 and mean) the k-means and the original labels\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Unsupervised DBSCAN \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='DBSCAN')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', dbscan.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that looks like crap. Let's try with other parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = cluster.DBSCAN(eps=0.5, metric='euclidean', min_samples=8)\n",
    "dbscan.fit(X)\n",
    "\n",
    "print(\"Number of clusters: \" + str(len(np.unique(dbscan.labels_))))\n",
    "\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Unsupervised DBSCAN \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='DBSCAN')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', dbscan.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Too many hyper parameters** that have a lot of impact on the outcome (which is **poor** in almost anycase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 6. Spectral Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = cluster.SpectralClustering(n_clusters=number_of_clusters,eigen_solver='arpack',affinity=\"nearest_neighbors\")\n",
    "spectral.fit(X)\n",
    "print(spectral.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting first three dimensions (i.e. std, std2 and mean) the k-means and the original labels\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Spectral Clustering \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='Spectral Clustering')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', spectral.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, what can we say...  Let's try other parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = cluster.SpectralClustering(n_clusters=number_of_clusters,eigen_solver='arpack',affinity=\"nearest_neighbors\",\n",
    "                                      n_init=10)\n",
    "spectral.fit(X)\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Spectral Clustering \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='Spectral Clustering')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', spectral.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 7. Try Hierarchical Clustering\n",
    "\n",
    "(see e.g. https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hierarchical = linkage(X.values, method='complete')\n",
    "labels = df['class'].tolist()\n",
    "\n",
    "# Plot a so calles 'dendrogram'\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "dendrogram(hierarchical,\n",
    "           labels=labels,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=12,           \n",
    "           )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is clearly separted (= with crystal),  A mosty (no crystal), but B (partial) is more fuzzy. Lets plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unclear however how to git this into a more suitable form to compare it with the original lables. Let's try the hierarchical clustering from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Affinity = {“euclidean”, “l1”, “l2”, “manhattan”, “cosine”}\n",
    "# Linkage = {“ward”, “complete”, “average”}\n",
    "\n",
    "Hclustering = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='complete')\n",
    "Hclustering.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into one dataframe:\n",
    "df2 = pd.concat([df, pd.Series(Hclustering.labels_)], axis=1)\n",
    "df2 = df2.rename(columns = { 0 : 'hc'})\n",
    "\n",
    "fig0 = plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Tileset 7 - Hierchical Clustering \",fontsize=14)\n",
    "\n",
    "ax = fig0.add_subplot(121, projection='3d', title='Hierarchical Clustering')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', Hclustering.labels_)\n",
    "\n",
    "ax = fig0.add_subplot(122, projection='3d', title='Original')\n",
    "scatter_3d(ax, df, '|img_mean|', '|img_std|', '|img_std2|', org_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** not bad **  (I tried a few combinations; 'complete' or 'average' with the 'cosine' metric gave best results)\n",
    "\n",
    "another one to try is 'feature aggoleration', which is combining hierarchical clustering with dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration\n",
    "agglo=FeatureAgglomeration(n_clusters=3).fit_transform(X)\n",
    "aggloX=agglo[:,0]\n",
    "aggloY=agglo[:,1]\n",
    "print(aggloX.shape, aggloY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it \n",
    "fig0 = plt.figure(figsize=(16, 12))\n",
    "ax = fig0.add_subplot(111, projection='3d')\n",
    "plt.suptitle(\"Tileset 7 - Feature Agglomeration\",fontsize=14)\n",
    "ax.scatter(agglo[:,0], agglo[:,1], agglo[:,2], c=org_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, I do not understand this very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 8. Comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a way to compare the results more quantively. Hard part is that the labels are different.\n",
    "As a first step, maybe just list the mean and count of the clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, pd.Series(k_means.labels_)], axis=1)\n",
    "df2 = df2.rename(columns = { 0 : 'k_means'})\n",
    "df2.head(1)\n",
    "print(df2.groupby(\"class\")[['|img_mean|', '|img_std|', '|img_std2|']].count())\n",
    "print(df2.groupby(\"k_means\")[['|img_mean|', '|img_std|', '|img_std2|']].count())\n",
    "print(df2.groupby(\"class\")[['|img_mean|', '|img_std|', '|img_std2|']].mean())\n",
    "print(df2.groupby(\"k_means\")[['|img_mean|', '|img_std|', '|img_std2|']].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* category A maps on cluster 1 of k-means\n",
    "* category B maps on cluster 0 of k-means (I think)\n",
    "* category C maps on cluster 2 of k-means (I think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably visualizing in the form of the heatmap is clearer to see it's effect.\n",
    "(as I cannot think of a way now how to properly quantify this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From SkLearn docs:_\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "\n",
    "2.3.9. Clustering performance evaluation\n",
    "\n",
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar that members of different classes according to some similarity metric.\n",
    "\n",
    "\n",
    "Many ways of scoring exist. Most point to the Adjusted Rand Index (ARI) as a good one. Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "k_means_pred = k_means.labels_\n",
    "spectral_pred = spectral.labels_\n",
    "dbscan_pred = dbscan.labels_\n",
    "hierarch_pred = Hclustering.labels_\n",
    "\n",
    "print('Adjusted Rand Index scoring:')\n",
    "print(\"k-means: %f\" % metrics.adjusted_rand_score(org_labels, k_means_pred))\n",
    "print(\"spectral: %f\" % metrics.adjusted_rand_score(org_labels, spectral_pred))\n",
    "print(\"dbscan: %f\" % metrics.adjusted_rand_score(org_labels, dbscan_pred))\n",
    "print(\"hierarchical: %f\" % metrics.adjusted_rand_score(org_labels, hierarch_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the ARI score, ** hierarchical clustering worked best ** on this dataset.\n",
    "\n",
    "Also try some of the other scoring methods:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorefunc =  metrics.adjusted_mutual_info_score\n",
    "print('Mutual Information Scoring:')\n",
    "\n",
    "print(\"k-means: %f\" % scorefunc(org_labels, k_means_pred))\n",
    "print(\"spectral: %f\" % scorefunc(org_labels, spectral_pred))\n",
    "print(\"dbscan: %f\" % scorefunc(org_labels, dbscan_pred))\n",
    "print(\"hierarchical: %f\" % scorefunc(org_labels, hierarch_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorefunc =  metrics.homogeneity_score\n",
    "print('Homogeneity Scoring:')\n",
    "\n",
    "print(\"k-means: %f\" % scorefunc(org_labels, k_means_pred))\n",
    "print(\"spectral: %f\" % scorefunc(org_labels, spectral_pred))\n",
    "print(\"dbscan: %f\" % scorefunc(org_labels, dbscan_pred))\n",
    "print(\"hierarchical: %f\" % scorefunc(org_labels, hierarch_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorefunc =  metrics.completeness_score\n",
    "print('Completeness Scoring:')\n",
    "\n",
    "print(\"k-means: %f\" % scorefunc(org_labels, k_means_pred))\n",
    "print(\"spectral: %f\" % scorefunc(org_labels, spectral_pred))\n",
    "print(\"dbscan: %f\" % scorefunc(org_labels, dbscan_pred))\n",
    "print(\"hierarchical: %f\" % scorefunc(org_labels, hierarch_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All these scoring metrics agree: hierarchical gave best results, followed by k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 9. Visualize unsupervised result as heatmap\n",
    "\n",
    "The numbers are still hard to interpret how good/bad it is. We need to visually check the result in the context of the actual image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unsupervised clustering results to the dataframe\n",
    "df3 = df\n",
    "df3 = pd.concat([df3, pd.Series(k_means_pred).rename('k_means')], axis=1)\n",
    "df3 = pd.concat([df3, pd.Series(dbscan_pred).rename('dbscan')], axis=1)\n",
    "df3 = pd.concat([df3, pd.Series(spectral_pred).rename('spectral')], axis=1)\n",
    "df3 = pd.concat([df3, pd.Series(hierarch_pred).rename('hierarch')], axis=1)\n",
    "\n",
    "df3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# see https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "colmap = 'RdYlGn'\n",
    "opac = 0.4\n",
    "figsize=(6,4)\n",
    "\n",
    "\"\"\"Show heatmaps of all images using the specified column as heats\"\"\"\n",
    "def show_heatmaps_allimgs(df_imgstats, heatcolname):\n",
    "    print('Heats from: ' + heatcolname)\n",
    "    imgnames = df_imgstats['filename'].unique()\n",
    "    for imgname in imgnames:\n",
    "        subimgs, heats = imgutils.getimgslices_fromdf(df_imgstats, imgname, heatcolname)\n",
    "        #rescale the heats to [0-1] range:\n",
    "        heats = (heats - np.min(heats)) / (np.max(heats)-np.min(heats))\n",
    "        print(imgname + ': ' + heatcolname)        \n",
    "        imgutils.showheatmap(subimgs, heats, heatdepend_opacity = False, opacity=opac, cmapname=colmap, title='image: ' + imgname, figsize=figsize)\n",
    "        print(heats)\n",
    "        \n",
    "\"\"\"Show multiple heatmaps of one image using different heats (as specified in heatcolnames\"\"\"\n",
    "def show_heatmap_multistats(df_imgstats, imgname, heatcolnames):    \n",
    "    print('Image: ' + imgname)\n",
    "    for colname in heatcolnames:\n",
    "        subimgs, heats = imgutils.getimgslices_fromdf(df_imgstats, imgname, colname)        \n",
    "        heats = (heats - np.min(heats)) / (np.max(heats)-np.min(heats))        \n",
    "        imgutils.showheatmap(subimgs, heats, heatdepend_opacity = False, opacity=opac,  cmapname=colmap, title='Heats from: ' + colname, figsize=figsize)\n",
    "        print(heats)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the techniques on first image\n",
    "imgnames = df3['filename'].unique()\n",
    "show_heatmap_multistats(df3, imgnames[0], ['|class|', 'hierarch', 'k_means', 'dbscan', 'spectral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only hierarchical clustering on all images\n",
    "show_heatmaps_allimgs(df3, 'hierarch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only k-means clustering on all images\n",
    "show_heatmaps_allimgs(df3, 'k_means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the visualization, it looked not that well.\n",
    "\n",
    "Need to manually count the number of positives, and false positives / false negatives to get a metric for this first unsupervised attempt. And see if I can combine multiple images in one view (makes the manual counting easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Show heatmaps of all images as one large image\"\"\"\n",
    "def show_large_heatmap(df_imgstats, heatcolname, imgnames, n_rows, n_cols, show_extra_info=False):\n",
    "        \n",
    "    assert len(imgnames) == n_rows * n_cols   \n",
    "    \n",
    "    # use first image to get the number of subimages per image\n",
    "    df_img1 = df_imgstats.loc[df['filename'] == imgnames[0]]    \n",
    "    n_y = df_img1.iloc[0]['n_y']\n",
    "    n_x = df_img1.iloc[0]['n_x']\n",
    "    \n",
    "    # grab all subimgs and heats into one large 2d array\n",
    "    i = 0\n",
    "    allsubimgs = np.empty((n_rows*n_y, n_cols*n_x), dtype=object)\n",
    "    allheats = np.empty((n_rows*n_y, n_cols*n_x), dtype=float)\n",
    "    for row in range(0,n_rows):\n",
    "        for col in range(0,n_cols):                            \n",
    "            imgname = imgnames[i]\n",
    "            subimgs, heats = imgutils.getimgslices_fromdf(df_imgstats, imgname, heatcolname)                        \n",
    "            for sub_row in range(0,n_y):                \n",
    "                for sub_col in range(0,n_x):\n",
    "                    all_row = row * n_y + sub_row\n",
    "                    all_col = col * n_x + sub_col                    \n",
    "                    allsubimgs[all_row, all_col] = subimgs[sub_row, sub_col]\n",
    "                    allheats[all_row, all_col] = heats[sub_row, sub_col]\n",
    "            \n",
    "            #print(heats.shape)\n",
    "            #print(heats)\n",
    "            #print(subimgs.shape)\n",
    "            #print(subimgs)\n",
    "                    \n",
    "            i = i + 1\n",
    "            \n",
    "    #rescale all heats to normalized range\n",
    "    allheats = (allheats - np.min(allheats)) / (np.max(allheats)-np.min(allheats))          \n",
    "    tittxt = 'Heats from: ' + heatcolname\n",
    "    imgutils.showheatmap(allsubimgs, allheats, heatdepend_opacity = False, opacity=opac, cmapname=colmap, title= tittxt, figsize=(12,10))\n",
    "    \n",
    "    # show info if requested\n",
    "    if show_extra_info:\n",
    "        print(allheats)\n",
    "        i=0;\n",
    "        for row in range(0,n_rows):\n",
    "            for col in range(0,n_cols):        \n",
    "                print(\"image %d at (%d , %d): %s\" % (i, row, col,imgnames[i]))\n",
    "\n",
    "    return (allsubimgs, allheats)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_classes = show_large_heatmap(df3, '|class|', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: the (semi)** manual labelling has some mistakes**. So for better analyses I need to fix the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_hierarch = show_large_heatmap(df3, 'hierarch', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['k_means'].replace({1 :2, 2:1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_kmeans = show_large_heatmap(df3, 'k_means', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remark **: The color coding can be misleading. The only thing that matters is that same types have same images\n",
    "(the cluster nr is used as the 'heat', but there is no ordering in this cluster numbering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's better than I thought initially, it was the visualization!\n",
    "(the individual heatmap used different color scales for each image, giving the wrong impression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: Add this large heatmap to imgutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 10. Get a baseline score (manual counting)\n",
    "So the real assessment can only be done manually by visual inspection, but counting the good ones and false positives / negatives is easy in large heatmaps.\n",
    "\n",
    "- It found all the 'full ones' (class C)\n",
    "- Tthe unsupervised learning did a good job on 'class B' if you agree that partial and the other texture are same class.\n",
    "- It missed only one in this class (19 of 20)\n",
    "- Interesting to check what it will do if we make it a two class problem or four class problem\n",
    "\n",
    "There are many ways how to express the performance, see https://en.wikipedia.org/wiki/Confusion_matrix. The confusion matrix is informative, but for metrics I can use some from (the block on the right). \n",
    "For the crystal case, we do not care too much about missing a few, so the True Positive Rate ('sensitivity) for the 'full ones' is a suitable indicator. The False Discovery Rate is alos intersting, as false positives are undesirable because they result in performing the next image acquisition experiment at places where there is not much to see.\n",
    "\n",
    "So, we use these:\n",
    "- TPR = True Positives / Real Positives\n",
    "- FDR = False Positives / (True Positives + False Positives)\n",
    "\n",
    "And then we want to see them for the 'Category C' i.e. with (almost) full crystal, and for partial\n",
    "\n",
    "Let's create some helper function for the counting and these scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_imgs_per_class(df_imgstats, classcolumn):\n",
    "    return df_imgstats[classcolumn].value_counts()\n",
    "\n",
    "def print_scores(methodname, class_count_tuples): \n",
    "    print(\"\")\n",
    "    print(\"{:<20}|{:^12}|{:^12}|\".format(methodname.upper(), \"True Pos\", \"False Pos\"))\n",
    "    print(\"-\"*(20+12+12+3))\n",
    "    \n",
    "    def print_score_line(class_name, TPR, FDR):\n",
    "        print(\"{:<20}|{:^12.2%}|{:^12.2%}| \".format(class_name, TPR, FDR ))  \n",
    "    \n",
    "    for (class_name, n_true_pos, n_false_pos, n_real_pos) in class_count_tuples:\n",
    "        TPR = n_true_pos/n_real_pos\n",
    "        FDR = n_false_pos/(n_true_pos + n_false_pos)\n",
    "        print_score_line(class_name, TPR, FDR)\n",
    "    \n",
    "    print(\"-\"*(20+12+12+3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores('Manual (using STD)', [('Full Crystal', 11, 2, 11),  ('Partial Crystal', 6, 4, 8) ])\n",
    "print_scores('Hierarchical', [('Full Crystal', 11, 1, 11), ('Partial Crystal', 7, 12, 8) ])\n",
    "print_scores('K-means', [('Full Crystal', 10, 1, 11), ('Partial Crystal', 7, 13, 8) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**: \n",
    "- some of the false positives in category 'Partial' are 'Full Ones' and some false positives in 'Full' are partial ones. So this score is a bit to strict, but accounting for this would require more complex scoring (or a full confusion matrix, where you still need to remark that some confusion is not so critical)\n",
    "- running the algorithms gives some variation, so these scores may deviate a bit (it is manually counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Try-out: what will PCA + hierarchical give?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['pca_1','pca_2','pca_3', 'pca_4', 'pca_5']\n",
    "\n",
    "n_comp = 3;\n",
    "\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into X Y vectors:\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hclustering_pca = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='complete')\n",
    "Hclustering_pca.fit(X_pca)\n",
    "hierarch_pca_pred = Hclustering_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df3, pd.Series(hierarch_pca_pred).rename('hierarch_pca')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_hierarch_pca = show_large_heatmap(df3, 'hierarch_pca', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Also see what happens with 5 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_comp = 5;\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]\n",
    "Hclustering_pca = AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='complete')\n",
    "Hclustering_pca.fit(X_pca)\n",
    "hierarch_pca_pred = Hclustering_pca.labels_\n",
    "\n",
    "df3 = pd.concat([df3, pd.Series(hierarch_pca_pred).rename('hierarch_pca_full')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_hierarch_pca_full = show_large_heatmap(df3, 'hierarch_pca_full', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is almost identical to the hierarchical clustering without PCA (and whether to use 3 or 5 components did not matter that much.\n",
    "\n",
    "Interesting of coarse with this PCA approach is that it will do feature selection for us\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's also try how it looks when going for 2 or 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5;\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]\n",
    "Hclustering_pca = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='complete')\n",
    "Hclustering_pca.fit(X_pca)\n",
    "hierarch_pca_pred = Hclustering_pca.labels_\n",
    "\n",
    "df3 = pd.concat([df3, pd.Series(hierarch_pca_pred).rename('hierarch_pca_2cats')], axis=1)\n",
    "hm_hierarch_pca_2cats = show_large_heatmap(df3, 'hierarch_pca_2cats', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Determining 2 classes perfectly finds the subimages without any features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5;\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]\n",
    "Hclustering_pca = AgglomerativeClustering(n_clusters=4, affinity='cosine', linkage='complete')\n",
    "Hclustering_pca.fit(X_pca)\n",
    "hierarch_pca_pred = Hclustering_pca.labels_\n",
    "\n",
    "df3 = pd.concat([df3, pd.Series(hierarch_pca_pred).rename('hierarch_pca_4cats')], axis=1)\n",
    "hm_hierarch_pca_4cats = show_large_heatmap(df3, 'hierarch_pca_4cats', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** With 4 classes, results are not improved as it separates the empty ones**\n",
    "\n",
    "(and not the partial from the other texture; so let's also try 5 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 5;\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]\n",
    "Hclustering_pca = AgglomerativeClustering(n_clusters=5, affinity='cosine', linkage='complete')\n",
    "Hclustering_pca.fit(X_pca)\n",
    "hierarch_pca_pred = Hclustering_pca.labels_\n",
    "\n",
    "df3 = pd.concat([df3, pd.Series(hierarch_pca_pred).rename('hierarch_pca_5cats')], axis=1)\n",
    "hm_hierarch_pca_5cats = show_large_heatmap(df3, 'hierarch_pca_5cats', imgnames[0:6], n_rows=2, n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it also does not get better with 5. ** So 3 it is for this data set! **\n",
    "\n",
    "Finally, let's also see how PCA plus k-means works out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.drop(columns=['k_means_pca'], inplace=True)\n",
    "\n",
    "n_comp = 3;\n",
    "pca = decomposition.TruncatedSVD(n_components=n_comp)\n",
    "X_fit = pca.fit_transform(X)\n",
    "df_pca = pd.DataFrame(X_fit[:,0:n_comp], columns=fieldnames[:n_comp])\n",
    "X_pca = df_pca.loc[:,fieldnames[:n_comp]]\n",
    "\n",
    "k_means_pca = cluster.KMeans(algorithm='auto', n_clusters=3, n_init=10, init='k-means++')\n",
    "k_means_pca.fit(X)\n",
    "\n",
    "k_means_pca_pred = k_means_pca.labels_\n",
    "\n",
    "df3 = pd.concat([df3, pd.Series(k_means_pca_pred).rename('k_means_pca')], axis=1)\n",
    "hm_kmeans_pca = show_large_heatmap(df3, 'k_means_pca', imgnames[0:6], n_rows=2, n_cols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical performs better in combination with k-means (there are quite some false pos. in the category B (partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## 12. Conclusions\n",
    "* On this data set, the **unsupervised learning** concept **worked quite well**, both with hierarchical or k-means clustering\n",
    "* Using PCA with** hierarchical clustering** gave similar results as the original features, with the advantage that it can reduced the number of required features\n",
    "* Best **assessment** of the quality at the moment is **by visualization and human inspection**\n",
    "* Scoring is difficult, but I have a **metric** now (though it involves manual counting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps:\n",
    "* move some functions into imgutils\n",
    "* combine the full pipeline into one script (with debugging options) so I can start trying this out on other data sets and play with parameters like 'number of sub images'\n",
    "* start thinking about hyper-parameter optimization \n",
    "\n",
    "\n",
    "Michael Janus, 27 July 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
